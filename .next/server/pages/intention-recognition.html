<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Intention Recognition by Amir Houieh</title><meta name="keywords" content="amir houieh, amirhouieh"/><meta name="description" content="Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips..."/><meta name="copyright" content="amir.cloud"/><meta name="language" content="EN"/><meta name="Classification" content="Design/Programming"/><meta name="author" content="Amir, amir.houieh@gmail.com"/><meta name="designer" content="amir houieh"/><meta name="owner" content="amir houieh"/><meta name="url" content="https://portfolio.amir.cloud/intention-recognition"/><meta name="identifier-URL" content="https://portfolio.amir.cloud/intention-recognition"/><meta property="og:title" content="Intention Recognition by Amir Houieh"/><meta property="og:url" content="https://portfolio.amir.cloud/intention-recognition"/><meta property="og:image" content="https://portfolio.amir.cloud/images/amir_houieh-lmKF5ZOrK_nYknRso-zys-1024.jpg"/><meta property="og:site_name" content="amir houieh"/><meta property="og:type" content="website"/><meta property="og:description" content="Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips..."/><meta name="twitter:image" content="https://portfolio.amir.cloud/images/amir_houieh-lmKF5ZOrK_nYknRso-zys-1024.jpg"/><meta itemProp="name" content="https://portfolio.amir.cloud/intention-recognition"/><meta itemProp="description" content="Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips..."/><meta itemProp="image" content="https://portfolio.amir.cloud/images/amir_houieh-lmKF5ZOrK_nYknRso-zys-1024.jpg"/><script type="application/ld+json">
              {
                &quot;@context&quot;: &quot;https://json-ld.org/contexts/person.jsonld&quot;,
                &quot;name&quot;: &quot;Amir Houieh&quot;,
                &quot;born&quot;: &quot;1987-03-22&quot;,
                &quot;url&quot;: &quot;amir.houieh&quot;,
                &quot;contactPoint&quot;: {
                  &quot;@type&quot;: &quot;ContactPoint&quot;,
                  &quot;availableLanguage&quot;: [&quot;English&quot;]
                },
                  &quot;sameAs&quot;: [
                  &quot;https://www.linkedin.com/in/amirhouieh&quot;,
                  &quot;https://github.com/amirhouieh&quot;,
                  &quot;https://vimeo.com/user13046302&quot;,
                  &quot;https://twitter.com/amirhouieh&quot;
                ]
              }
        </script><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/89f6c9f565961e37.css" as="style"/><link rel="stylesheet" href="/_next/static/css/89f6c9f565961e37.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-96470c7b0317e27e.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-a16c617c42bd88d0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-3e399e5cc9feb1c1.js" defer=""></script><script src="/_next/static/chunks/63-071348b73b9c6dd3.js" defer=""></script><script src="/_next/static/chunks/984-a8b69ab87f1229c4.js" defer=""></script><script src="/_next/static/chunks/4-0cf1ea086bbc82f7.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-25e06b2eebb16a85.js" defer=""></script><script src="/_next/static/aqAUvMRKsTzW4luGvoG2f/_buildManifest.js" defer=""></script><script src="/_next/static/aqAUvMRKsTzW4luGvoG2f/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><div class="blur" font-size="36" style="text-shadow:0 0 50px blue;color:transparent;display:inline-block"><a href="/"><h2 class="page-info">/</h2></a><a title="about me" href="https://amir.cloud"><small>amir houieh</small></a></div><br/><br/><br/><div class="project container"><div><div class="page-header"><a style="display:none" href="/intention-recognition/"></a><code class="date">2019</code><h2 class="page-info">Intention Recognition</h2><div class="tags"><i>AI</i>, <i>computer-vision</i>, <i>gesture recognition</i>, <i>HCI</i></div><div class="stack"><span>OpenPose</span><span>, </span><span>dlib</span><span>, </span><span>YOLOv3</span><span>, </span><span>LSTM</span><span>, </span><span>TensorFlow</span><span>, </span><span>TensorRT</span><span>, </span><span>WebRTC</span><span>, </span><span>Jetson Nano</span><span>, </span><span>Raspberry Pi</span></div><a href="https://suslib.com/core/intention-recognition" target="_blank" class="external-link">https://suslib.com/core/intention-recognition</a></div><div class="page-hero"><figure class="horizontal"><a href="javascript:;" title="amir houieh - Intention Recognition-intention-recognition.jpg"><img src="../images/amir_houieh-lmKF5ZOrK_nYknRso-zys-320.jpg" alt="amir houieh - Intention Recognition-intention-recognition.jpg" title="amir houieh - Intention Recognition-intention-recognition.jpg" srcSet="../images/amir_houieh-lmKF5ZOrK_nYknRso-zys-320.jpg 320w, ../images/amir_houieh-lmKF5ZOrK_nYknRso-zys-640.jpg 640w, ../images/amir_houieh-lmKF5ZOrK_nYknRso-zys-1024.jpg 1024w"/></a></figure></div><div class="page-info"><p>If my KR project was about making collections smart, Intention Recognition was about making them playful. We wanted people to control digital systems with nothing more than gestures and the way they handled objects. Pick up a book, wave your hand, flip through pages — and the system responds.</p>
<p>Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips. I jumped in on dataset curation too, which was both painful and weirdly satisfying.</p>
<p>What made IR special was combining gestures with object interactions. It wasn&#39;t just about waving at a camera — it was about how you handled things in the real world. We even built an &quot;observer mode&quot; where the system could notice new gestures and adapt over time. In demos, people loved teaching it something on the spot and seeing it respond.</p>
<p>We made it work on cheap hardware: regular webcams streaming over WebRTC, models optimized with TensorRT to run on Jetson Nanos and Raspberry Pis. No special sensors, no Kinect-style setup — just software magic.</p>
<p>IR ended up being showcased in libraries, exhibitions, and design festivals. It was the project that made people literally laugh and smile in front of our booth, waving their hands around like kids. For me, it was proof that futuristic interaction doesn&#39;t need futuristic hardware — just the right mix of vision, scrappiness, and teamwork.</p>
<p><code>with</code> <a href="https://suslib.com">Martijn de Heer</a>, <a href="https://suslib.com">Homayoun Moradi</a></p>
</div></div><div class="page-images"><figure class="horizontal"><a href="javascript:;" title="amir houieh - Intention Recognition-image1.png"><img src="../images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-320.png" alt="amir houieh - Intention Recognition-image1.png" title="amir houieh - Intention Recognition-image1.png" srcSet="../images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-320.png 320w, ../images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-640.png 640w, ../images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-1024.png 1024w"/></a></figure><figure class="horizontal"><a href="javascript:;" title="amir houieh - Intention Recognition-image2.jpg"><img src="../images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-320.jpg" alt="amir houieh - Intention Recognition-image2.jpg" title="amir houieh - Intention Recognition-image2.jpg" srcSet="../images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-320.jpg 320w, ../images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-640.jpg 640w, ../images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-1024.jpg 1024w"/></a></figure></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"project":{"html":"\u003cp\u003eIf my KR project was about making collections smart, Intention Recognition was about making them playful. We wanted people to control digital systems with nothing more than gestures and the way they handled objects. Pick up a book, wave your hand, flip through pages — and the system responds.\u003c/p\u003e\n\u003cp\u003eTechnically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips. I jumped in on dataset curation too, which was both painful and weirdly satisfying.\u003c/p\u003e\n\u003cp\u003eWhat made IR special was combining gestures with object interactions. It wasn\u0026#39;t just about waving at a camera — it was about how you handled things in the real world. We even built an \u0026quot;observer mode\u0026quot; where the system could notice new gestures and adapt over time. In demos, people loved teaching it something on the spot and seeing it respond.\u003c/p\u003e\n\u003cp\u003eWe made it work on cheap hardware: regular webcams streaming over WebRTC, models optimized with TensorRT to run on Jetson Nanos and Raspberry Pis. No special sensors, no Kinect-style setup — just software magic.\u003c/p\u003e\n\u003cp\u003eIR ended up being showcased in libraries, exhibitions, and design festivals. It was the project that made people literally laugh and smile in front of our booth, waving their hands around like kids. For me, it was proof that futuristic interaction doesn\u0026#39;t need futuristic hardware — just the right mix of vision, scrappiness, and teamwork.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ewith\u003c/code\u003e \u003ca href=\"https://suslib.com\"\u003eMartijn de Heer\u003c/a\u003e, \u003ca href=\"https://suslib.com\"\u003eHomayoun Moradi\u003c/a\u003e\u003c/p\u003e\n","link":"https://suslib.com/core/intention-recognition","title":"Intention Recognition","description":"\u003cp\u003eTechnically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips. I jumped in on dataset curation too, which was both painful and weirdly satisfying.\u003c/p\u003e\n","blurb":"AI system for gesture and object interaction using computer vision to control digital systems through natural hand movements and gestures.","tags":["AI","computer-vision","gesture recognition","HCI"],"slug":"intention-recognition","thumb":{"src":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-320.jpg","srcSet":[{"path":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-320.jpg","size":320},{"path":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-640.jpg","size":640},{"path":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-1024.jpg","size":1024}],"caption":null,"alt":"Intention Recognition-intention-recognition.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-320.png","srcSet":[{"path":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-320.png","size":320},{"path":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-640.png","size":640},{"path":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-1024.png","size":1024}],"caption":null,"alt":"Intention Recognition-image1.png","order":-1,"r":1.5},{"src":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-320.jpg","srcSet":[{"path":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-320.jpg","size":320},{"path":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-640.jpg","size":640},{"path":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-1024.jpg","size":1024}],"caption":null,"alt":"Intention Recognition-image2.jpg","order":-1,"r":1.5}],"stack":["OpenPose","dlib","YOLOv3","LSTM","TensorFlow","TensorRT","WebRTC","Jetson Nano","Raspberry Pi"],"dateString":"2019","dataYear":2019,"videos":[]}},"__N_SSG":true},"page":"/[slug]","query":{"slug":"intention-recognition"},"buildId":"aqAUvMRKsTzW4luGvoG2f","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>