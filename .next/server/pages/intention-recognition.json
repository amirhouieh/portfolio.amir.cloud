{"pageProps":{"project":{"html":"<p>If my KR project was about making collections smart, Intention Recognition was about making them playful. We wanted people to control digital systems with nothing more than gestures and the way they handled objects. Pick up a book, wave your hand, flip through pages — and the system responds.</p>\n<p>Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips. I jumped in on dataset curation too, which was both painful and weirdly satisfying.</p>\n<p>What made IR special was combining gestures with object interactions. It wasn&#39;t just about waving at a camera — it was about how you handled things in the real world. We even built an &quot;observer mode&quot; where the system could notice new gestures and adapt over time. In demos, people loved teaching it something on the spot and seeing it respond.</p>\n<p>We made it work on cheap hardware: regular webcams streaming over WebRTC, models optimized with TensorRT to run on Jetson Nanos and Raspberry Pis. No special sensors, no Kinect-style setup — just software magic.</p>\n<p>IR ended up being showcased in libraries, exhibitions, and design festivals. It was the project that made people literally laugh and smile in front of our booth, waving their hands around like kids. For me, it was proof that futuristic interaction doesn&#39;t need futuristic hardware — just the right mix of vision, scrappiness, and teamwork.</p>\n<p><code>with</code> <a href=\"https://suslib.com\">Martijn de Heer</a>, <a href=\"https://suslib.com\">Homayoun Moradi</a></p>\n","link":"https://suslib.com/core/intention-recognition","title":"Intention Recognition","description":"<p>Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips. I jumped in on dataset curation too, which was both painful and weirdly satisfying.</p>\n","blurb":"AI system for gesture and object interaction using computer vision to control digital systems through natural hand movements and gestures.","tags":["AI","computer-vision","gesture recognition","HCI"],"slug":"intention-recognition","thumb":{"src":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-320.jpg","srcSet":[{"path":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-320.jpg","size":320},{"path":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-640.jpg","size":640},{"path":"images/amir_houieh-lmKF5ZOrK_nYknRso-zys-1024.jpg","size":1024}],"caption":null,"alt":"Intention Recognition-intention-recognition.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-320.png","srcSet":[{"path":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-320.png","size":320},{"path":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-640.png","size":640},{"path":"images/amir_houieh-D8IU9Q4u2qB-6vaoqgVMc-1024.png","size":1024}],"caption":null,"alt":"Intention Recognition-image1.png","order":-1,"r":1.5},{"src":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-320.jpg","srcSet":[{"path":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-320.jpg","size":320},{"path":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-640.jpg","size":640},{"path":"images/amir_houieh-ay5nvpIInsdfC7EgJhjfy-1024.jpg","size":1024}],"caption":null,"alt":"Intention Recognition-image2.jpg","order":-1,"r":1.5}],"stack":["OpenPose","dlib","YOLOv3","LSTM","TensorFlow","TensorRT","WebRTC","Jetson Nano","Raspberry Pi"],"dateString":"2019","dataYear":2019,"videos":[]}},"__N_SSG":true}