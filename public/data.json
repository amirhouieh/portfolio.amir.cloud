{"currentProjects":[{"html":"<p>Hiro is a market place for generative and interactive artworks.</p>\n","link":"https://hir.ooo","title":"HIRO","description":"<p>Hiro is a market place for generative and interactive artworks.</p>\n","blurb":"Marketplace for generative and interactive artworks.","tags":["SAAS","Online platform","Creative coding"],"current":true,"slug":"hiro","thumb":{"src":"images/amir_houieh-dwUs7Web657K0am-glJAa-320.png","srcSet":[{"path":"images/amir_houieh-dwUs7Web657K0am-glJAa-320.png","size":320},{"path":"images/amir_houieh-dwUs7Web657K0am-glJAa-640.png","size":640},{"path":"images/amir_houieh-dwUs7Web657K0am-glJAa-1024.png","size":1024}],"caption":null,"alt":"HIRO-logox.png","order":-1,"r":1.5},"images":[],"stack":null,"dateString":"2021","dataYear":2021,"videos":[]},{"html":"<p><a href=\"https://suslib.com\">SUSLIB</a> aims to build the future of knowledge interaction combining computational design, research and A.I. </p>\n","link":"https://suslib.com","title":"SUSLIB","description":"<p><a href=\"https://suslib.com\">SUSLIB</a> aims to build the future of knowledge interaction combining computational design, research and A.I. </p>\n","blurb":"R&D lab building the future of knowledge interaction through computational design, research, and AI.","tags":["A.I. software","digital/physical interactions","SAAS","Machine learning","Future of knowledge"],"current":true,"slug":"suslib","thumb":{"src":"images/amir_houieh-3JEt21KFOJhA2MfWg2okg-320.jpg","srcSet":[{"path":"images/amir_houieh-3JEt21KFOJhA2MfWg2okg-320.jpg","size":320},{"path":"images/amir_houieh-3JEt21KFOJhA2MfWg2okg-640.jpg","size":640},{"path":"images/amir_houieh-3JEt21KFOJhA2MfWg2okg-1024.jpg","size":1024}],"caption":null,"alt":"SUSLIB-1617215097-index-cover-autonomous-library-centered-light.jpg","order":1,"r":1.5},"images":[],"stack":null,"dateString":"2018","dataYear":2018,"videos":[]},{"html":"<p><a href=\"https://unbody.io\">Unbody</a> is a damn easy API for advanced AI, from chatbots to generative search, for your private data—whether it&#39;s on Google Drive or Slack, in any format from PDFs to spreadsheets to videos—all via a single GraphQl touchpoint.</p>\n","link":"https://unbody.io","title":"Unbody","description":"<p><a href=\"https://unbody.io\">Unbody</a> is a damn easy API for advanced AI, from chatbots to generative search, for your private data—whether it&#39;s on Google Drive or Slack, in any format from PDFs to spreadsheets to videos—all via a single GraphQl touchpoint.</p>\n","blurb":"API for advanced AI (chatbots, generative search, RAG) that works with your private data from Google Drive, Slack, PDFs, videos—all via GraphQL.","tags":["A.I. software","SAAS","Machine learning","LLM"],"current":true,"slug":"unbody","thumb":{"src":"images/amir_houieh-a0lpDgoQbJavhgRfpclzR-320.jpg","srcSet":[{"path":"images/amir_houieh-a0lpDgoQbJavhgRfpclzR-320.jpg","size":320},{"path":"images/amir_houieh-a0lpDgoQbJavhgRfpclzR-640.jpg","size":640},{"path":"images/amir_houieh-a0lpDgoQbJavhgRfpclzR-1024.jpg","size":1024}],"caption":null,"alt":"Unbody-Frame 25.jpg","order":-1,"r":1.5},"images":[],"stack":null,"dateString":"2018","dataYear":2018,"videos":[]}],"archivedProjects":[{"html":"<p><a href=\"https://took.wiki\">took.wiki</a></p>\n<p>TOOK is an online reader generator that uses Wikipedia as its knowledge source. You can upload a text file (e.g: an article, a book, etc), and TOOK creates a new narrative based on important/relevant entities and keywords contextualized with Wikipedia articles.</p>\n<p>Furthermore, you could also publish your reader with ❤️ and so it becomes available for everyone else.</p>\n<p>The current version uses exact same algorithm for entity extraction, relevancy evaluation but is different in Technology. In the background, there is no use of advanced technologies such as NLP. It is purely the results of a semantic system and mechanism written in code.</p>\n<h4>Background</h4>\n<p>Took is an upgraded version of an old design research project about the future of reading with the title of &quot;Tomorrow Book&quot;. Read more <a href=\"https://portfolio.amir.cloud/tomorrow-book/\">here</a></p>\n<h4>Adiotional note</h4>\n","title":"TOOK.wiki","description":"<p>The current version uses exact same algorithm for entity extraction, relevancy evaluation but is different in Technology. In the background, there is no use of advanced technologies such as NLP. It is purely the results of a semantic system and mechanism written in code.</p>\n","blurb":"Online reader generator using Wikipedia as knowledge source—creates new narratives from your text based on entity extraction.","tags":["research","online platform","Experimental publishing","Indie making"],"slug":"took.wiki","thumb":{"src":"images/amir_houieh--agLCDhI5qaRWxjQA96B1-320.png","srcSet":[{"path":"images/amir_houieh--agLCDhI5qaRWxjQA96B1-320.png","size":320},{"path":"images/amir_houieh--agLCDhI5qaRWxjQA96B1-640.png","size":640},{"path":"images/amir_houieh--agLCDhI5qaRWxjQA96B1-1024.png","size":1024}],"caption":null,"alt":"TOOK.wiki-tomorrow-book.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-WZFD9KEpg5uY3FNEcj4Is-320.jpg","srcSet":[{"path":"images/amir_houieh-WZFD9KEpg5uY3FNEcj4Is-320.jpg","size":320},{"path":"images/amir_houieh-WZFD9KEpg5uY3FNEcj4Is-640.jpg","size":640},{"path":"images/amir_houieh-WZFD9KEpg5uY3FNEcj4Is-1024.jpg","size":1024}],"caption":null,"alt":"TOOK.wiki-5-image7.jpg","order":5,"r":1.5}],"stack":["Typescript","Next.js","Firebase"],"dateString":"2021","dataYear":2021,"videos":[],"order":10001},{"html":"<p>Gray is an AI-native, open-source blogging framework that makes reading more contextual and dynamic.</p>\n<p>AI has transformed writing, and now it&#39;s time to enhance reading. Gray is an AI-native, open-source blogging framework built in NextJs and powered by Unbody.io. Gray makes reading more contextual and dynamic. Designed for both content creators and readers, Gray brings articles, podcasts and video blogs to life, turning reading into an interactive, personalized experience.</p>\n<h2>Purpose</h2>\n<p><strong>Exploration and Experimentation</strong>: Gray uses AI to explore new ways of engaging readers, adapting content to their context and preferences.</p>\n<p><strong>Dynamic and Contextual Reading</strong>: Gray aims to personalize the reading experience, making content feel specifically tailored and relevant to each user.</p>\n<h2>Features</h2>\n<p><strong>AI-enabled content creation</strong>: Gray generates metadata and required context based on the blog&#39;s content for various purposes such as SEO and search prompts enhancement.</p>\n<p><strong>AI-enabled content curation</strong>: Gray categorizes blog posts into creatively constructed categories to promote exploration.</p>\n<p><strong>Semantic search</strong>: Users can search through the posts and within a post using natural language.</p>\n<p><strong>Generative search</strong>: Users can use the search bar to search, explore, or even chat with the blog. Gray can handle any form of query in natural language, whether it&#39;s a simple concept or a complex task. Some example queries include:</p>\n<ul>\n<li>Comparing Amir&#39;s posts about AI native apps with Tomas&#39;s presentation.</li>\n<li>Querying the future of mobility.</li>\n<li>Asking about the main topics on the blog.</li>\n<li>Seeking reading suggestions based on a positive sentiment about AI.</li>\n</ul>\n","link":"https://github.com/unbody-io/Gray","title":"Gray","description":"<p>AI has transformed writing, and now it&#39;s time to enhance reading. Gray is an AI-native, open-source blogging framework built in NextJs and powered by Unbody.io. Gray makes reading more contextual and dynamic. Designed for both content creators and readers, Gray brings articles, podcasts and video blogs to life, turning reading into an interactive, personalized experience.</p>\n","blurb":"AI-native blogging framework that makes reading contextual and dynamic with semantic search and generative content exploration.","tags":["AI-native","blogging framework","open-source","contextual reading"],"slug":"gray","thumb":{"src":"images/amir_houieh-RMirZBqE_55oqJZZD_gUs-320.png","srcSet":[{"path":"images/amir_houieh-RMirZBqE_55oqJZZD_gUs-320.png","size":320},{"path":"images/amir_houieh-RMirZBqE_55oqJZZD_gUs-640.png","size":640},{"path":"images/amir_houieh-RMirZBqE_55oqJZZD_gUs-1024.png","size":1024}],"caption":null,"alt":"Gray-gray.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-NRZN2n4hiD5otqCPMRl2m-320.png","srcSet":[{"path":"images/amir_houieh-NRZN2n4hiD5otqCPMRl2m-320.png","size":320},{"path":"images/amir_houieh-NRZN2n4hiD5otqCPMRl2m-640.png","size":640},{"path":"images/amir_houieh-NRZN2n4hiD5otqCPMRl2m-1024.png","size":1024}],"caption":null,"alt":"Gray-image1.png","order":-1,"r":1.5},{"src":"images/amir_houieh-1TI4Ak6AL84U51BcBjWB6-320.png","srcSet":[{"path":"images/amir_houieh-1TI4Ak6AL84U51BcBjWB6-320.png","size":320},{"path":"images/amir_houieh-1TI4Ak6AL84U51BcBjWB6-640.png","size":640},{"path":"images/amir_houieh-1TI4Ak6AL84U51BcBjWB6-1024.png","size":1024}],"caption":null,"alt":"Gray-image2.png","order":-1,"r":1.5},{"src":"images/amir_houieh-uh1rixtyZKywm73sxhLiB-320.png","srcSet":[{"path":"images/amir_houieh-uh1rixtyZKywm73sxhLiB-320.png","size":320},{"path":"images/amir_houieh-uh1rixtyZKywm73sxhLiB-640.png","size":640},{"path":"images/amir_houieh-uh1rixtyZKywm73sxhLiB-1024.png","size":1024}],"caption":null,"alt":"Gray-image3.png","order":-1,"r":1.5}],"stack":["Unbody","NextJs","ShadCN"],"dateString":"2024","dataYear":2024,"videos":[]},{"html":"<p>Autonomous Libraries (AL) was an R&amp;D project where we asked: what would a library look like if we designed it from scratch for the age of digital knowledge production? Instead of one big central building, AL imagined a distributed network of hybrid knowledge spaces — modular, self-managed, community-run, and spread across everyday environments like coffeehouses, squares, or train stations.</p>\n<p>We combined all the core technologies we had developed at Suslib — Knowledge Recognition for semantic search and contextual linking, Intention Recognition for gesture- and object-based interaction, and lightweight AR interfaces to connect digital and physical resources. The result was a new kind of space: one where you could pick up a book and instantly see its digital context, annotate a page with thoughts and images, or continue reading seamlessly on your device after leaving.</p>\n<p>As a team, we treated AL as both a design research exercise and a technological prototype. I led the engineering direction, ensuring our APIs and ML systems could integrate into a hybrid physical-digital environment, while collaborating closely with Martijn on interaction design and Studio Helioripple on modular architectural systems.</p>\n<p>AL was never meant to be a single app or product, but a vision of community-driven, resilient, non-commercial knowledge infrastructure — one that could exist anywhere in the city, belong to everyone, and evolve with new forms of knowledge production.</p>\n<p><code>with</code> <a href=\"https://suslib.com\">Martijn de Heer</a> (co-founder, designer), <a href=\"https://suslib.com\">Studio Helioripple</a> (Amin Bahrami)</p>\n","link":"https://suslib.com/research/autonomous-libraries","title":"Autonomous Libraries","description":"<p>We combined all the core technologies we had developed at Suslib — Knowledge Recognition for semantic search and contextual linking, Intention Recognition for gesture- and object-based interaction, and lightweight AR interfaces to connect digital and physical resources. The result was a new kind of space: one where you could pick up a book and instantly see its digital context, annotate a page with thoughts and images, or continue reading seamlessly on your device after leaving.</p>\n","blurb":"R&D project reimagining libraries as distributed networks of hybrid knowledge spaces with AI-powered semantic search and AR interfaces.","tags":["R&D","future-of-knowledge","smart-city","IoT"],"slug":"autonomous-libraries","thumb":{"src":"images/amir_houieh-anqY_pW2g0wFqtX-QIyKV-320.png","srcSet":[{"path":"images/amir_houieh-anqY_pW2g0wFqtX-QIyKV-320.png","size":320},{"path":"images/amir_houieh-anqY_pW2g0wFqtX-QIyKV-640.png","size":640},{"path":"images/amir_houieh-anqY_pW2g0wFqtX-QIyKV-1024.png","size":1024}],"caption":null,"alt":"Autonomous Libraries-autonomous-libraries.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-d8q3nGFjH1FLaN1znehhS-320.png","srcSet":[{"path":"images/amir_houieh-d8q3nGFjH1FLaN1znehhS-320.png","size":320},{"path":"images/amir_houieh-d8q3nGFjH1FLaN1znehhS-640.png","size":640},{"path":"images/amir_houieh-d8q3nGFjH1FLaN1znehhS-1024.png","size":1024}],"caption":null,"alt":"Autonomous Libraries-image1.png","order":-1,"r":1.5},{"src":"images/amir_houieh-R4xZc13Ek6GxQwJKldDlJ-320.png","srcSet":[{"path":"images/amir_houieh-R4xZc13Ek6GxQwJKldDlJ-320.png","size":320},{"path":"images/amir_houieh-R4xZc13Ek6GxQwJKldDlJ-640.png","size":640},{"path":"images/amir_houieh-R4xZc13Ek6GxQwJKldDlJ-1024.png","size":1024}],"caption":null,"alt":"Autonomous Libraries-image2.png","order":-1,"r":1.5},{"src":"images/amir_houieh-1761477346187image3.gif","srcSet":[{"path":"images/amir_houieh-1761477346187image3.gif","size":600}],"caption":null,"alt":"Autonomous Libraries-image3.gif","order":-1,"r":1.5}],"stack":["Word2Vec","Elasticsearch","OCR","CV","NLP","OpenPose","YOLOv3","LSTMs","TensorFlow","AR interfaces","modular architecture","HCI"],"dateString":"2020","dataYear":2020,"videos":[]},{"html":"<p>IIA started as a side experiment to test what our Knowledge Recognition API could do in practice — and quickly grew into a full product. The idea was simple but powerful: point your tablet&#39;s camera at an object in a collection, and instantly know whether it&#39;s already in the inventory, check it in or out, or add it with enriched metadata generated on the fly.</p>\n<p>We built IIA for Android and iOS tablets with a <strong>camera-first design</strong>. Instead of clunky forms or RFID scanners, the AR camera was the main interface. It recognized objects, suggested descriptions using ML, and made collection management as easy as taking a photo. Under the hood it leaned on our KR API — combining Word2Vec + Elasticsearch for semantic search with computer vision models to identify assets visually.</p>\n<p>I led the product engineering and development team through multiple iterations: from concept (literally training on the combined bookcases of our team) to web-based prototypes, and finally to the beta version that people could register to try. By 2022, we launched version 1.0 for both iOS and Android, shaped heavily by user feedback.</p>\n<p>IIA was the first time we turned our research into a <strong>working, usable product</strong>. It proved that our backend technology could actually make life easier for real people managing collections — librarians, archivists, or anyone dealing with physical and digital assets. For me, it was a huge milestone: going from experimental APIs to a polished, camera-driven app that people could actually use.</p>\n<p><code>with</code> <a href=\"https://suslib.com\">Martijn de Heer</a></p>\n","link":"https://suslib.com/solution/inventory-assistant","title":"IIA (Intelligence Inventory Assistant)","description":"<p>We built IIA for Android and iOS tablets with a <strong>camera-first design</strong>. Instead of clunky forms or RFID scanners, the AR camera was the main interface. It recognized objects, suggested descriptions using ML, and made collection management as easy as taking a photo. Under the hood it leaned on our KR API — combining Word2Vec + Elasticsearch for semantic search with computer vision models to identify assets visually.</p>\n","blurb":"AR mobile app for inventory management using camera-first design and ML to identify objects and generate metadata on the fly.","tags":["AR","ML","inventory management","mobile app"],"slug":"iia-(intelligence-inventory-assistant)","thumb":{"src":"images/amir_houieh-7ydvITY6cniLYGoaM6Si8-320.png","srcSet":[{"path":"images/amir_houieh-7ydvITY6cniLYGoaM6Si8-320.png","size":320},{"path":"images/amir_houieh-7ydvITY6cniLYGoaM6Si8-640.png","size":640},{"path":"images/amir_houieh-7ydvITY6cniLYGoaM6Si8-1024.png","size":1024}],"caption":null,"alt":"IIA (Intelligence Inventory Assistant)-inventory-assistant-application.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-KkU8Eo1zHrsnZjBUQe3Ax-320.png","srcSet":[{"path":"images/amir_houieh-KkU8Eo1zHrsnZjBUQe3Ax-320.png","size":320},{"path":"images/amir_houieh-KkU8Eo1zHrsnZjBUQe3Ax-640.png","size":640},{"path":"images/amir_houieh-KkU8Eo1zHrsnZjBUQe3Ax-1024.png","size":1024}],"caption":null,"alt":"IIA (Intelligence Inventory Assistant)-image1.png","order":-1,"r":1.5},{"src":"images/amir_houieh-4prANAt-ccd1S9TuXah9L-320.png","srcSet":[{"path":"images/amir_houieh-4prANAt-ccd1S9TuXah9L-320.png","size":320},{"path":"images/amir_houieh-4prANAt-ccd1S9TuXah9L-640.png","size":640},{"path":"images/amir_houieh-4prANAt-ccd1S9TuXah9L-1024.png","size":1024}],"caption":null,"alt":"IIA (Intelligence Inventory Assistant)-image2.png","order":-1,"r":1.5}],"stack":["ARKit/ARCore","Knowledge Recognition API","TensorFlow","Elasticsearch","REST API"],"dateString":"2020","dataYear":2020,"videos":[]},{"html":"<p><a href=\"https://triennale2019.hetnieuweinstituut.nl\">triennale2019.hetnieuweinstituut.nl</a></p>\n<p><code>with</code> <a href=\"https://studiomoniker.com\"><code>Studio Moniker</code></a></p>\n<p>Small but exciting project in collaboration with Studio Moniker. This is an interactive typographic webcover designed by Rudy Guedj, and programmed by me. (made by p5JS and opentype-js).</p>\n","title":"I See That I See What You Do not See","description":"<p>Small but exciting project in collaboration with Studio Moniker. This is an interactive typographic webcover designed by Rudy Guedj, and programmed by me. (made by p5JS and opentype-js).</p>\n","blurb":"Interactive typographic web cover combining generative design with real-time interaction—collaboration with Studio Moniker.","tags":["generative typography","interactive typography"],"slug":"i-see-that-i-see-what-you-do-not-see","thumb":{"src":"images/amir_houieh-1761477346068ezgif-2-5d1a1d20bc1c.gif","srcSet":[{"path":"images/amir_houieh-1761477346068ezgif-2-5d1a1d20bc1c.gif","size":600}],"caption":null,"alt":"I See That I See What You Do not See-ezgif-2-5d1a1d20bc1c.gif","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-1761477346116ezgif-2-000c12fab057.gif","srcSet":[{"path":"images/amir_houieh-1761477346116ezgif-2-000c12fab057.gif","size":600}],"caption":null,"alt":"I See That I See What You Do not See-ezgif-2-000c12fab057.gif","order":-1,"r":1.5},{"src":"images/amir_houieh-1761477346119ezgif-2-77f92584bbca.gif","srcSet":[{"path":"images/amir_houieh-1761477346119ezgif-2-77f92584bbca.gif","size":600}],"caption":null,"alt":"I See That I See What You Do not See-ezgif-2-77f92584bbca.gif","order":-1,"r":1.5},{"src":"images/amir_houieh-1761477346122ezgif-2-c9dbd1cd888a.gif","srcSet":[{"path":"images/amir_houieh-1761477346122ezgif-2-c9dbd1cd888a.gif","size":600}],"caption":null,"alt":"I See That I See What You Do not See-ezgif-2-c9dbd1cd888a.gif","order":-1,"r":1.5},{"src":"images/amir_houieh-1761477346122ezgif-2-f435cc0528b8.gif","srcSet":[{"path":"images/amir_houieh-1761477346122ezgif-2-f435cc0528b8.gif","size":600}],"caption":null,"alt":"I See That I See What You Do not See-ezgif-2-f435cc0528b8.gif","order":-1,"r":1.5}],"stack":["es6","html/css","p5js","webpack"],"dateString":"2019","dataYear":2019,"videos":[]},{"html":"<p>If my KR project was about making collections smart, Intention Recognition was about making them playful. We wanted people to control digital systems with nothing more than gestures and the way they handled objects. Pick up a book, wave your hand, flip through pages — and the system responds.</p>\n<p>Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips. I jumped in on dataset curation too, which was both painful and weirdly satisfying.</p>\n<p>What made IR special was combining gestures with object interactions. It wasn&#39;t just about waving at a camera — it was about how you handled things in the real world. We even built an &quot;observer mode&quot; where the system could notice new gestures and adapt over time. In demos, people loved teaching it something on the spot and seeing it respond.</p>\n<p>We made it work on cheap hardware: regular webcams streaming over WebRTC, models optimized with TensorRT to run on Jetson Nanos and Raspberry Pis. No special sensors, no Kinect-style setup — just software magic.</p>\n<p>IR ended up being showcased in libraries, exhibitions, and design festivals. It was the project that made people literally laugh and smile in front of our booth, waving their hands around like kids. For me, it was proof that futuristic interaction doesn&#39;t need futuristic hardware — just the right mix of vision, scrappiness, and teamwork.</p>\n<p><code>with</code> <a href=\"https://suslib.com\">Martijn de Heer</a>, <a href=\"https://suslib.com\">Homayoun Moradi</a></p>\n","link":"https://suslib.com/core/intention-recognition","title":"Intention Recognition","description":"<p>Technically, it was a fun mess. We used OpenPose for skeleton tracking, dlib for facial gestures, YOLOv3 for object detection, and LSTMs in TensorFlow to capture sequences of movement. I helped design the system and API, while our ML engineer went deep into dataset training — over 100k labeled clips. I jumped in on dataset curation too, which was both painful and weirdly satisfying.</p>\n","blurb":"AI system for gesture and object interaction using computer vision to control digital systems through natural hand movements and gestures.","tags":["AI","computer-vision","gesture recognition","HCI"],"slug":"intention-recognition","thumb":{"src":"images/amir_houieh--4UNIVr6v15rn0WR7UAft-320.jpg","srcSet":[{"path":"images/amir_houieh--4UNIVr6v15rn0WR7UAft-320.jpg","size":320},{"path":"images/amir_houieh--4UNIVr6v15rn0WR7UAft-640.jpg","size":640},{"path":"images/amir_houieh--4UNIVr6v15rn0WR7UAft-1024.jpg","size":1024}],"caption":null,"alt":"Intention Recognition-intention-recognition.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-7qfwpi2V9TU4vyNfH9U-T-320.png","srcSet":[{"path":"images/amir_houieh-7qfwpi2V9TU4vyNfH9U-T-320.png","size":320},{"path":"images/amir_houieh-7qfwpi2V9TU4vyNfH9U-T-640.png","size":640},{"path":"images/amir_houieh-7qfwpi2V9TU4vyNfH9U-T-1024.png","size":1024}],"caption":null,"alt":"Intention Recognition-image1.png","order":-1,"r":1.5},{"src":"images/amir_houieh-XwCh5ySFpuc-oEOHBx5s_-320.jpg","srcSet":[{"path":"images/amir_houieh-XwCh5ySFpuc-oEOHBx5s_-320.jpg","size":320},{"path":"images/amir_houieh-XwCh5ySFpuc-oEOHBx5s_-640.jpg","size":640},{"path":"images/amir_houieh-XwCh5ySFpuc-oEOHBx5s_-1024.jpg","size":1024}],"caption":null,"alt":"Intention Recognition-image2.jpg","order":-1,"r":1.5}],"stack":["OpenPose","dlib","YOLOv3","LSTM","TensorFlow","TensorRT","WebRTC","Jetson Nano","Raspberry Pi"],"dateString":"2019","dataYear":2019,"videos":[]},{"html":"<p>KR was our first big experiment at <a href=\"https://suslib.com\">Suslib</a>. The idea was simple: what if a collection of books, images, or files could actually understand itself? Instead of being a flat archive, it would be a network of meaning that you could search semantically and explore contextually.</p>\n<p>This was 2018, so we didn&#39;t have transformer models or ready-made semantic search APIs. Instead, we pieced it together ourselves: YOLOv3 and Mask R-CNN for object detection, Tesseract for OCR, spaCy for entity recognition, LDA for topics, and TextRank for summarization. For semantics we trained Word2Vec embeddings and dropped them into Elasticsearch to give collections a &quot;search by idea&quot; capability. It felt like hacking the future together with duct tape and GPUs.</p>\n<p>I led the team that built it — backend, ML, and MLOps engineers — and my role was to drive the product architecture, glue all the moving parts, and also get my hands dirty with model training and pipeline design. We wrapped it all into a REST and streaming API that sat neatly between a CMS and database. That made it lightweight, flexible, and fast enough to power our own AR inventory app while still being useful for outside developers.</p>\n<p>Looking back, KR was the project that put us on the map: it got us into Dutch Design Week, helped us secure funding, and proved that we could build practical AI-driven knowledge systems before the ecosystem was ready for it.</p>\n<p><code>with</code> <a href=\"https://suslib.com\">Martijn de Heer</a>, <a href=\"https://suslib.com\">Homayoun Moradi</a></p>\n","link":"https://suslib.com/core/knowledge-recognition","title":"Knowledge Recognition","description":"<p>This was 2018, so we didn&#39;t have transformer models or ready-made semantic search APIs. Instead, we pieced it together ourselves: YOLOv3 and Mask R-CNN for object detection, Tesseract for OCR, spaCy for entity recognition, LDA for topics, and TextRank for summarization. For semantics we trained Word2Vec embeddings and dropped them into Elasticsearch to give collections a &quot;search by idea&quot; capability. It felt like hacking the future together with duct tape and GPUs.</p>\n","blurb":"AI-powered semantic search system that transforms collections into networks of meaning using computer vision and NLP.","tags":["AI","semantic-search","ML","knowledge systems"],"slug":"knowledge-recognition","thumb":{"src":"images/amir_houieh-x9jwZZEBxfQbuneZdnBs8-320.png","srcSet":[{"path":"images/amir_houieh-x9jwZZEBxfQbuneZdnBs8-320.png","size":320},{"path":"images/amir_houieh-x9jwZZEBxfQbuneZdnBs8-640.png","size":640},{"path":"images/amir_houieh-x9jwZZEBxfQbuneZdnBs8-1024.png","size":1024}],"caption":null,"alt":"Knowledge Recognition-knowledge-recognition.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-d-TUPfBoBbA0Ih73VLRpW-320.png","srcSet":[{"path":"images/amir_houieh-d-TUPfBoBbA0Ih73VLRpW-320.png","size":320},{"path":"images/amir_houieh-d-TUPfBoBbA0Ih73VLRpW-640.png","size":640},{"path":"images/amir_houieh-d-TUPfBoBbA0Ih73VLRpW-1024.png","size":1024}],"caption":null,"alt":"Knowledge Recognition-image1.png","order":-1,"r":1.5},{"src":"images/amir_houieh-HiFPS8bn2LU8YmkKTxjsA-320.png","srcSet":[{"path":"images/amir_houieh-HiFPS8bn2LU8YmkKTxjsA-320.png","size":320},{"path":"images/amir_houieh-HiFPS8bn2LU8YmkKTxjsA-640.png","size":640},{"path":"images/amir_houieh-HiFPS8bn2LU8YmkKTxjsA-1024.png","size":1024}],"caption":null,"alt":"Knowledge Recognition-image2.png","order":-1,"r":1.5},{"src":"images/amir_houieh-e0UqEiBBEDEz_QqC6UhQl-320.png","srcSet":[{"path":"images/amir_houieh-e0UqEiBBEDEz_QqC6UhQl-320.png","size":320},{"path":"images/amir_houieh-e0UqEiBBEDEz_QqC6UhQl-640.png","size":640},{"path":"images/amir_houieh-e0UqEiBBEDEz_QqC6UhQl-1024.png","size":1024}],"caption":null,"alt":"Knowledge Recognition-image3.png","order":-1,"r":1.5},{"src":"images/amir_houieh-96fmzgk-elKCSid9UbZ1W-320.png","srcSet":[{"path":"images/amir_houieh-96fmzgk-elKCSid9UbZ1W-320.png","size":320},{"path":"images/amir_houieh-96fmzgk-elKCSid9UbZ1W-640.png","size":640},{"path":"images/amir_houieh-96fmzgk-elKCSid9UbZ1W-1024.png","size":1024}],"caption":null,"alt":"Knowledge Recognition-image4.png","order":-1,"r":1.5},{"src":"images/amir_houieh-qGNE1RBQYl3XPq4Y3x-Wr-320.png","srcSet":[{"path":"images/amir_houieh-qGNE1RBQYl3XPq4Y3x-Wr-320.png","size":320},{"path":"images/amir_houieh-qGNE1RBQYl3XPq4Y3x-Wr-640.png","size":640},{"path":"images/amir_houieh-qGNE1RBQYl3XPq4Y3x-Wr-1024.png","size":1024}],"caption":null,"alt":"Knowledge Recognition-image5.png","order":-1,"r":1.5}],"stack":["YOLOv3","Mask R-CNN","Tesseract OCR","spaCy","LDA","Word2Vec","Elasticsearch","TensorFlow","PyTorch","REST API","Streaming API"],"dateString":"2018","dataYear":2018,"videos":[]},{"html":"<p><a href=\"https://openrndr.org\">openrndr.org</a>\nThis is one of the projects which I worked on at RNDR.</p>\n<p>Website for the creative coding framework OPENRNDR.</p>\n","title":"OPENRNDR.org","description":"<p><a href=\"https://openrndr.org\">openrndr.org</a>\nThis is one of the projects which I worked on at RNDR.</p>\n","blurb":"Website for the creative coding framework OPENRNDR.","tags":["web application"],"slug":"openrndr.org","thumb":{"src":"images/amir_houieh-gcLCYK_w10fBR1Y89n5RR-320.png","srcSet":[{"path":"images/amir_houieh-gcLCYK_w10fBR1Y89n5RR-320.png","size":320},{"path":"images/amir_houieh-gcLCYK_w10fBR1Y89n5RR-640.png","size":640},{"path":"images/amir_houieh-gcLCYK_w10fBR1Y89n5RR-1024.png","size":1024}],"caption":null,"alt":"OPENRNDR.org-Screen Shot 2019-03-12 at 16.39.46.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-mx_13_rtiIDI19Zn17rz7-320.png","srcSet":[{"path":"images/amir_houieh-mx_13_rtiIDI19Zn17rz7-320.png","size":320},{"path":"images/amir_houieh-mx_13_rtiIDI19Zn17rz7-640.png","size":640},{"path":"images/amir_houieh-mx_13_rtiIDI19Zn17rz7-1024.png","size":1024}],"caption":null,"alt":"OPENRNDR.org-Screen Shot 2019-03-12 at 16.35.41.png","order":-1,"r":1.5},{"src":"images/amir_houieh-pdqieN97dz9XOmJWyK2n3-320.png","srcSet":[{"path":"images/amir_houieh-pdqieN97dz9XOmJWyK2n3-320.png","size":320},{"path":"images/amir_houieh-pdqieN97dz9XOmJWyK2n3-640.png","size":640},{"path":"images/amir_houieh-pdqieN97dz9XOmJWyK2n3-1024.png","size":1024}],"caption":null,"alt":"OPENRNDR.org-Screen Shot 2019-03-12 at 16.35.47.png","order":-1,"r":1.5},{"src":"images/amir_houieh-mxyYfghSHRlNCg9Stjsea-320.png","srcSet":[{"path":"images/amir_houieh-mxyYfghSHRlNCg9Stjsea-320.png","size":320},{"path":"images/amir_houieh-mxyYfghSHRlNCg9Stjsea-640.png","size":640},{"path":"images/amir_houieh-mxyYfghSHRlNCg9Stjsea-1024.png","size":1024}],"caption":null,"alt":"OPENRNDR.org-Screen Shot 2019-03-12 at 16.35.53.png","order":-1,"r":1.5},{"src":"images/amir_houieh-diXDDq5plAsTkI6-2hhy3-320.png","srcSet":[{"path":"images/amir_houieh-diXDDq5plAsTkI6-2hhy3-320.png","size":320},{"path":"images/amir_houieh-diXDDq5plAsTkI6-2hhy3-640.png","size":640},{"path":"images/amir_houieh-diXDDq5plAsTkI6-2hhy3-1024.png","size":1024}],"caption":"phone version (menu).png","alt":"phone version (menu).png","order":-1,"r":1.5},{"src":"images/amir_houieh-hwWTS3B_FFPXmazhuRWex-320.png","srcSet":[{"path":"images/amir_houieh-hwWTS3B_FFPXmazhuRWex-320.png","size":320},{"path":"images/amir_houieh-hwWTS3B_FFPXmazhuRWex-640.png","size":640},{"path":"images/amir_houieh-hwWTS3B_FFPXmazhuRWex-1024.png","size":1024}],"caption":"phone-version.png","alt":"phone-version.png","order":-1,"r":1.5}],"stack":["es6","html/css","TypeScript","React","Netlify","react-static","DatoCMS"],"dateString":"2018","dataYear":2018,"videos":[]},{"html":"<p><a href=\"https://artexyz.info\">project webpage</a></p>\n<p><code>at</code> <a href=\"https://www.artez.nl/en/course/graphic-design\"><code>Artez Graphic design department</code></a>,</p>\n<p>Rexperimental Framework happened as a 4 days workshop given by amir houieh  with BA students from Graphic Design Arnhem, in October 2018 at Artez. The workshop aimed to discuss and develop experimental publishing frameworks in our digital realm based on decentralized and distributed networks. The workshop consist of 4 lectures and one project assignment. The workshop was an extension to REPUB project. The lectures introduces topics such as content flow in digital environments, the fundamental of the web and decentralized networks like p2p.</p>\n<p>As the assignment students needed to design an alternative model for content production and consumption as well as distribution in form a publishing framework. In other words they needed to design how users could produce and manage their content as well as how this content is published through a an interface. The distribution part of framework must occur on a decentralized-distributed network.\nAt the end the workshop yields into several different projects which you can read more about on the website of the workshop.</p>\n","title":"Rexperimental Framework","description":"<p>Rexperimental Framework happened as a 4 days workshop given by amir houieh  with BA students from Graphic Design Arnhem, in October 2018 at Artez. The workshop aimed to discuss and develop experimental publishing frameworks in our digital realm based on decentralized and distributed networks. The workshop consist of 4 lectures and one project assignment. The workshop was an extension to REPUB project. The lectures introduces topics such as content flow in digital environments, the fundamental of the web and decentralized networks like p2p.</p>\n","blurb":"4-day workshop on experimental publishing frameworks based on decentralized networks—exploring p2p content distribution.","tags":["workshop","Lecture"],"slug":"rexperimental-framework","thumb":{"src":"images/amir_houieh-UOifL5RQ8nm3fByqp1INg-320.jpg","srcSet":[{"path":"images/amir_houieh-UOifL5RQ8nm3fByqp1INg-320.jpg","size":320},{"path":"images/amir_houieh-UOifL5RQ8nm3fByqp1INg-640.jpg","size":640},{"path":"images/amir_houieh-UOifL5RQ8nm3fByqp1INg-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-artexyz.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-1ukB6ZQdtcyJhXecMgu_P-320.jpg","srcSet":[{"path":"images/amir_houieh-1ukB6ZQdtcyJhXecMgu_P-320.jpg","size":320},{"path":"images/amir_houieh-1ukB6ZQdtcyJhXecMgu_P-640.jpg","size":640},{"path":"images/amir_houieh-1ukB6ZQdtcyJhXecMgu_P-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-1F2XGIUnhF-UrFXFfV8Ou5Z0RtnHj2MWP.jpg","order":1,"r":1.5},{"src":"images/amir_houieh-17614773463291rNHxcSIRJxluRAzciBw0Sk2UHUmUVEGI.gif","srcSet":[{"path":"images/amir_houieh-17614773463291rNHxcSIRJxluRAzciBw0Sk2UHUmUVEGI.gif","size":600}],"caption":null,"alt":"Rexperimental Framework-1rNHxcSIRJxluRAzciBw0Sk2UHUmUVEGI.gif","order":1,"r":1.5},{"src":"images/amir_houieh-m3EKcx4HUfZltlMDdSuHS-320.jpg","srcSet":[{"path":"images/amir_houieh-m3EKcx4HUfZltlMDdSuHS-320.jpg","size":320},{"path":"images/amir_houieh-m3EKcx4HUfZltlMDdSuHS-640.jpg","size":640},{"path":"images/amir_houieh-m3EKcx4HUfZltlMDdSuHS-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-20181009_140856.jpg","order":2,"r":1.5},{"src":"images/amir_houieh--t9VXtJUsXu_mCiXq1rWX-320.jpg","srcSet":[{"path":"images/amir_houieh--t9VXtJUsXu_mCiXq1rWX-320.jpg","size":320},{"path":"images/amir_houieh--t9VXtJUsXu_mCiXq1rWX-640.jpg","size":640},{"path":"images/amir_houieh--t9VXtJUsXu_mCiXq1rWX-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-20181009_161147_HDR.jpg","order":2,"r":1.5},{"src":"images/amir_houieh-jPsKMHB-rNVTlnuCszGYY-320.jpg","srcSet":[{"path":"images/amir_houieh-jPsKMHB-rNVTlnuCszGYY-320.jpg","size":320},{"path":"images/amir_houieh-jPsKMHB-rNVTlnuCszGYY-640.jpg","size":640},{"path":"images/amir_houieh-jPsKMHB-rNVTlnuCszGYY-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-20181009_164447_HDR.jpg","order":2,"r":1.5},{"src":"images/amir_houieh-ly_IKabme83VkFpiavduL-320.jpg","srcSet":[{"path":"images/amir_houieh-ly_IKabme83VkFpiavduL-320.jpg","size":320},{"path":"images/amir_houieh-ly_IKabme83VkFpiavduL-640.jpg","size":640},{"path":"images/amir_houieh-ly_IKabme83VkFpiavduL-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-20181011_171942_HDR.jpg","order":2,"r":1.5},{"src":"images/amir_houieh-rSuUpwnZ_zR21_jwBgWsw-320.jpg","srcSet":[{"path":"images/amir_houieh-rSuUpwnZ_zR21_jwBgWsw-320.jpg","size":320},{"path":"images/amir_houieh-rSuUpwnZ_zR21_jwBgWsw-640.jpg","size":640},{"path":"images/amir_houieh-rSuUpwnZ_zR21_jwBgWsw-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-20181011_183205.jpg","order":2,"r":1.5},{"src":"images/amir_houieh-6CbinzVdCNgaNK7Xk1K6Y-320.jpg","srcSet":[{"path":"images/amir_houieh-6CbinzVdCNgaNK7Xk1K6Y-320.jpg","size":320},{"path":"images/amir_houieh-6CbinzVdCNgaNK7Xk1K6Y-640.jpg","size":640},{"path":"images/amir_houieh-6CbinzVdCNgaNK7Xk1K6Y-1024.jpg","size":1024}],"caption":null,"alt":"Rexperimental Framework-White board - concept.jpg","order":-1,"r":1.5}],"stack":null,"dateString":"2016","dataYear":2016,"videos":[]},{"html":"<p>Exhibited at Salone del Mobile, Milano</p>\n<p><code>with</code> <a href=\"http://www.minddesign.info/\"><code>Niels Schrader</code></a>, <a href=\"https://krks.info/\"><code>Gabor Kerekes</code></a>, <a href=\"http://veravandeseyp.com\"><code>Vera van de Seyp</code></a></p>\n<p>The installation Running Out by Graphic Design students Amir Houieh, Gábor Kerekes, and <a href=\"http://veravandeseyp.com\">Vera van de Seyp</a>, confronts its audience with the consequences of the rapidly increasing electricity consumption. In their research, the students address one of the biggest challenge of the 21st century: the global dependecy on electricity.\nThe work is centered around a dismantled computer screen that displays the CO2 emissions generated by the device itself and a countdown revealing the days left until the world runs out of natural resources (numbers based on projections published at COP21).\nIn fall 2015, the Climate Change Conference took place in Paris. Delegates of all United Nations member countries met to discuss urgent arrangement in relation of climate change. Their discussion resulted in the Paris Agreement that made 196 countries agree upon changing national and international policies for the sake of keeping the damage for the climate as low as possible.\nIf the average temperature in the world rises above 2ºC, the damage to the environment is unstoppable and the climate will grow into a vicious circle leading to a rapidly increasing amount of natural disasters. Right now, the average temperature rise is already at 1.5ºC and rising steadily. The largest contribution to this increase is the burning of fossil fuels to generate electrical energy.\nAbout a third of the carbon emissions that are generated is created in a domestic environment. This means that all of us contribute with our daily habits.\nThe interfaces through which people get in contact with their energy consumption leaves out all context: the carbon emissions generated by the device, in the home, but also the bigger context. The global change isn’t visible in the home and the time that is left before the 2ºC temperature elevation is crossed partially because of these daily domestic useages.\nWith their project, Houieh, Kerekes and Van de Seyp aim to lay bare how common energy interfaces seem to deliberately cover the urgency of the global energy crisis.</p>\n<p><em>Pictures by Sem Langendijk and Roel Backaert.</em></p>\n","title":"Running Out","description":"<p>The installation Running Out by Graphic Design students Amir Houieh, Gábor Kerekes, and <a href=\"http://veravandeseyp.com\">Vera van de Seyp</a>, confronts its audience with the consequences of the rapidly increasing electricity consumption. In their research, the students address one of the biggest challenge of the 21st century: the global dependecy on electricity.\nThe work is centered around a dismantled computer screen that displays the CO2 emissions generated by the device itself and a countdown revealing the days left until the world runs out of natural resources (numbers based on projections published at COP21).\nIn fall 2015, the Climate Change Conference took place in Paris. Delegates of all United Nations member countries met to discuss urgent arrangement in relation of climate change. Their discussion resulted in the Paris Agreement that made 196 countries agree upon changing national and international policies for the sake of keeping the damage for the climate as low as possible.\nIf the average temperature in the world rises above 2ºC, the damage to the environment is unstoppable and the climate will grow into a vicious circle leading to a rapidly increasing amount of natural disasters. Right now, the average temperature rise is already at 1.5ºC and rising steadily. The largest contribution to this increase is the burning of fossil fuels to generate electrical energy.\nAbout a third of the carbon emissions that are generated is created in a domestic environment. This means that all of us contribute with our daily habits.\nThe interfaces through which people get in contact with their energy consumption leaves out all context: the carbon emissions generated by the device, in the home, but also the bigger context. The global change isn’t visible in the home and the time that is left before the 2ºC temperature elevation is crossed partially because of these daily domestic useages.\nWith their project, Houieh, Kerekes and Van de Seyp aim to lay bare how common energy interfaces seem to deliberately cover the urgency of the global energy crisis.</p>\n","blurb":"Installation confronting audiences with electricity consumption and CO2 emissions—exhibited at Salone del Mobile, Milano.","tags":["real-time data","installation","sustainability"],"slug":"running-out","thumb":{"src":"images/amir_houieh-XOo5OrgsVh9eiUn930V9P-320.jpg","srcSet":[{"path":"images/amir_houieh-XOo5OrgsVh9eiUn930V9P-320.jpg","size":320},{"path":"images/amir_houieh-XOo5OrgsVh9eiUn930V9P-640.jpg","size":640},{"path":"images/amir_houieh-XOo5OrgsVh9eiUn930V9P-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-running-out-project.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-8tAZNAsCKmiNoPDuGZL92-320.jpg","srcSet":[{"path":"images/amir_houieh-8tAZNAsCKmiNoPDuGZL92-320.jpg","size":320},{"path":"images/amir_houieh-8tAZNAsCKmiNoPDuGZL92-640.jpg","size":640},{"path":"images/amir_houieh-8tAZNAsCKmiNoPDuGZL92-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-1-reset-5.jpg","order":1,"r":1.5},{"src":"images/amir_houieh-bCRPUkv1019rl9NvbCQbY-320.jpg","srcSet":[{"path":"images/amir_houieh-bCRPUkv1019rl9NvbCQbY-320.jpg","size":320},{"path":"images/amir_houieh-bCRPUkv1019rl9NvbCQbY-640.jpg","size":640},{"path":"images/amir_houieh-bCRPUkv1019rl9NvbCQbY-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-2-reset-7.jpg","order":2,"r":1.5},{"src":"images/amir_houieh-itclY3gB2kPk0oHGVm_yy-320.jpg","srcSet":[{"path":"images/amir_houieh-itclY3gB2kPk0oHGVm_yy-320.jpg","size":320},{"path":"images/amir_houieh-itclY3gB2kPk0oHGVm_yy-640.jpg","size":640},{"path":"images/amir_houieh-itclY3gB2kPk0oHGVm_yy-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-3-reset-8.jpg","order":3,"r":1.5},{"src":"images/amir_houieh-heTpCsPowIqzD51Wp_oHI-320.jpg","srcSet":[{"path":"images/amir_houieh-heTpCsPowIqzD51Wp_oHI-320.jpg","size":320},{"path":"images/amir_houieh-heTpCsPowIqzD51Wp_oHI-640.jpg","size":640},{"path":"images/amir_houieh-heTpCsPowIqzD51Wp_oHI-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-4-reset-9.jpg","order":4,"r":1.5},{"src":"images/amir_houieh-jdRtfbrYD0yyKg1MoIH9O-320.jpg","srcSet":[{"path":"images/amir_houieh-jdRtfbrYD0yyKg1MoIH9O-320.jpg","size":320},{"path":"images/amir_houieh-jdRtfbrYD0yyKg1MoIH9O-640.jpg","size":640},{"path":"images/amir_houieh-jdRtfbrYD0yyKg1MoIH9O-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-5-reset-10.jpg","order":5,"r":1.5},{"src":"images/amir_houieh-ql-gSuhDEgKmjPmUxqhan-320.jpg","srcSet":[{"path":"images/amir_houieh-ql-gSuhDEgKmjPmUxqhan-320.jpg","size":320},{"path":"images/amir_houieh-ql-gSuhDEgKmjPmUxqhan-640.jpg","size":640},{"path":"images/amir_houieh-ql-gSuhDEgKmjPmUxqhan-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-6-reset-11.jpg","order":6,"r":1.5},{"src":"images/amir_houieh-joC1A3yoZSXmJ_WsnDX44-320.jpg","srcSet":[{"path":"images/amir_houieh-joC1A3yoZSXmJ_WsnDX44-320.jpg","size":320},{"path":"images/amir_houieh-joC1A3yoZSXmJ_WsnDX44-640.jpg","size":640},{"path":"images/amir_houieh-joC1A3yoZSXmJ_WsnDX44-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-7-reset-12.jpg","order":7,"r":1.5},{"src":"images/amir_houieh-ketBIqmii032z9QB1Eft--320.jpg","srcSet":[{"path":"images/amir_houieh-ketBIqmii032z9QB1Eft--320.jpg","size":320},{"path":"images/amir_houieh-ketBIqmii032z9QB1Eft--640.jpg","size":640},{"path":"images/amir_houieh-ketBIqmii032z9QB1Eft--1024.jpg","size":1024}],"caption":null,"alt":"Running Out-8-reset-6.jpg","order":8,"r":1.5},{"src":"images/amir_houieh-cvpMqabUVCx-Tbg5bl_-c-320.jpg","srcSet":[{"path":"images/amir_houieh-cvpMqabUVCx-Tbg5bl_-c-320.jpg","size":320},{"path":"images/amir_houieh-cvpMqabUVCx-Tbg5bl_-c-640.jpg","size":640},{"path":"images/amir_houieh-cvpMqabUVCx-Tbg5bl_-c-1024.jpg","size":1024}],"caption":null,"alt":"Running Out-9-reset-13.jpg","order":9,"r":1.5}],"stack":null,"dateString":"2016","dataYear":2016,"videos":[]},{"html":"<p><a href=\"https://github.com/amirhouieh/type-pixelator\">source code on Github</a>\n<a href=\"https://archive.amir.cloud/pixelator/\">archive.amir.cloud/pixelator</a></p>\n<p>A simple <a href=\"http://www.drawbot.com\">Drawbot</a> module, pixelating texts.</p>\n","title":"Pixelator","description":"<p><a href=\"https://github.com/amirhouieh/type-pixelator\">source code on Github</a>\n<a href=\"https://archive.amir.cloud/pixelator/\">archive.amir.cloud/pixelator</a></p>\n","blurb":"Drawbot module for pixelating text—exploring generative typography.","tags":["generative type","alternative tool"],"slug":"pixelator","thumb":{"src":"images/amir_houieh-8EM45dGw9UBIzkkyNzFJL-320.png","srcSet":[{"path":"images/amir_houieh-8EM45dGw9UBIzkkyNzFJL-320.png","size":320},{"path":"images/amir_houieh-8EM45dGw9UBIzkkyNzFJL-640.png","size":640},{"path":"images/amir_houieh-8EM45dGw9UBIzkkyNzFJL-1024.png","size":1024}],"caption":null,"alt":"Pixelator-16-2x.png","order":1,"r":1.5},"images":[{"src":"images/amir_houieh-zFr1Wv-xz1TErVMZz0P7t-320.png","srcSet":[{"path":"images/amir_houieh-zFr1Wv-xz1TErVMZz0P7t-320.png","size":320},{"path":"images/amir_houieh-zFr1Wv-xz1TErVMZz0P7t-640.png","size":640},{"path":"images/amir_houieh-zFr1Wv-xz1TErVMZz0P7t-1024.png","size":1024}],"caption":null,"alt":"Pixelator-16-2x.png","order":1,"r":1.5},{"src":"images/amir_houieh-7cObcWRYjogfOK2SRyRDm-320.png","srcSet":[{"path":"images/amir_houieh-7cObcWRYjogfOK2SRyRDm-320.png","size":320},{"path":"images/amir_houieh-7cObcWRYjogfOK2SRyRDm-640.png","size":640},{"path":"images/amir_houieh-7cObcWRYjogfOK2SRyRDm-1024.png","size":1024}],"caption":null,"alt":"Pixelator-17-2x.png","order":1,"r":1.5},{"src":"images/amir_houieh-YjVJoFLxCS3TGVXUPXxkT-320.png","srcSet":[{"path":"images/amir_houieh-YjVJoFLxCS3TGVXUPXxkT-320.png","size":320},{"path":"images/amir_houieh-YjVJoFLxCS3TGVXUPXxkT-640.png","size":640},{"path":"images/amir_houieh-YjVJoFLxCS3TGVXUPXxkT-1024.png","size":1024}],"caption":null,"alt":"Pixelator-20-2x.png","order":2,"r":1.5},{"src":"images/amir_houieh-VQd97GOwgJQ9kPxj_LKf2-320.png","srcSet":[{"path":"images/amir_houieh-VQd97GOwgJQ9kPxj_LKf2-320.png","size":320},{"path":"images/amir_houieh-VQd97GOwgJQ9kPxj_LKf2-640.png","size":640},{"path":"images/amir_houieh-VQd97GOwgJQ9kPxj_LKf2-1024.png","size":1024}],"caption":null,"alt":"Pixelator-21-2x.png","order":2,"r":1.5},{"src":"images/amir_houieh-l4jCpEQW2uVZnjIt_Gtj3-320.png","srcSet":[{"path":"images/amir_houieh-l4jCpEQW2uVZnjIt_Gtj3-320.png","size":320},{"path":"images/amir_houieh-l4jCpEQW2uVZnjIt_Gtj3-640.png","size":640},{"path":"images/amir_houieh-l4jCpEQW2uVZnjIt_Gtj3-1024.png","size":1024}],"caption":null,"alt":"Pixelator-23-2x.png","order":2,"r":1.5},{"src":"images/amir_houieh-PdCXDq_cOKbOh8LGUgKya-320.png","srcSet":[{"path":"images/amir_houieh-PdCXDq_cOKbOh8LGUgKya-320.png","size":320},{"path":"images/amir_houieh-PdCXDq_cOKbOh8LGUgKya-640.png","size":640},{"path":"images/amir_houieh-PdCXDq_cOKbOh8LGUgKya-1024.png","size":1024}],"caption":null,"alt":"Pixelator-5-2x.png","order":5,"r":1.5},{"src":"images/amir_houieh-f_P-p9yvcWyw4jK8mWMWp-320.png","srcSet":[{"path":"images/amir_houieh-f_P-p9yvcWyw4jK8mWMWp-320.png","size":320},{"path":"images/amir_houieh-f_P-p9yvcWyw4jK8mWMWp-640.png","size":640},{"path":"images/amir_houieh-f_P-p9yvcWyw4jK8mWMWp-1024.png","size":1024}],"caption":null,"alt":"Pixelator-8-2x.png","order":8,"r":1.5}],"stack":["python","drawbot"],"dateString":"2015","dataYear":2015,"videos":[]},{"html":"<p><a href=\"https://tomorrow-book.amir.cloud\">demo</a></p>\n<p>The Tomorrow Book, an experimental research project in form of an school assignment, is one of my early attempts to experiment with text driven design. Within this assignment, I studied various types of content containers in a digital environment and questioned the different reading experiences digital and analogue readers have to offer. As result, the Tomorrow Book was transferred onto a website where it claims that “a new container requires new content”. This website uses Wikipedia articles in order to generate new reading narratives for books.</p>\n","title":"Tomorrow Book","description":"<p>The Tomorrow Book, an experimental research project in form of an school assignment, is one of my early attempts to experiment with text driven design. Within this assignment, I studied various types of content containers in a digital environment and questioned the different reading experiences digital and analogue readers have to offer. As result, the Tomorrow Book was transferred onto a website where it claims that “a new container requires new content”. This website uses Wikipedia articles in order to generate new reading narratives for books.</p>\n","blurb":"Experimental website that generates new reading narratives from Wikipedia articles—exploring digital vs analog reading experiences.","tags":["research","website"],"slug":"tomorrow-book","thumb":{"src":"images/amir_houieh-YNvY3M-kmWq8GxxV_1eMh-320.png","srcSet":[{"path":"images/amir_houieh-YNvY3M-kmWq8GxxV_1eMh-320.png","size":320},{"path":"images/amir_houieh-YNvY3M-kmWq8GxxV_1eMh-640.png","size":640},{"path":"images/amir_houieh-YNvY3M-kmWq8GxxV_1eMh-1024.png","size":1024}],"caption":null,"alt":"Tomorrow Book-tomorrow-book.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-p46YT7vidETA3yoUMh47J-320.jpg","srcSet":[{"path":"images/amir_houieh-p46YT7vidETA3yoUMh47J-320.jpg","size":320},{"path":"images/amir_houieh-p46YT7vidETA3yoUMh47J-640.jpg","size":640},{"path":"images/amir_houieh-p46YT7vidETA3yoUMh47J-1024.jpg","size":1024}],"caption":null,"alt":"Tomorrow Book-1-image2.jpg","order":1,"r":1.5},{"src":"images/amir_houieh-ut9SXxI_uPD3kuYGAFRFP-320.jpg","srcSet":[{"path":"images/amir_houieh-ut9SXxI_uPD3kuYGAFRFP-320.jpg","size":320},{"path":"images/amir_houieh-ut9SXxI_uPD3kuYGAFRFP-640.jpg","size":640},{"path":"images/amir_houieh-ut9SXxI_uPD3kuYGAFRFP-1024.jpg","size":1024}],"caption":null,"alt":"Tomorrow Book-2-image3.jpg","order":2,"r":1.5},{"src":"images/amir_houieh-DmzQ45TMKI7dGdBZhb3Qw-320.jpg","srcSet":[{"path":"images/amir_houieh-DmzQ45TMKI7dGdBZhb3Qw-320.jpg","size":320},{"path":"images/amir_houieh-DmzQ45TMKI7dGdBZhb3Qw-640.jpg","size":640},{"path":"images/amir_houieh-DmzQ45TMKI7dGdBZhb3Qw-1024.jpg","size":1024}],"caption":null,"alt":"Tomorrow Book-3-image5.jpg","order":3,"r":1.5},{"src":"images/amir_houieh-UlQ9CZ1VDQHcqgoulY080-320.jpg","srcSet":[{"path":"images/amir_houieh-UlQ9CZ1VDQHcqgoulY080-320.jpg","size":320},{"path":"images/amir_houieh-UlQ9CZ1VDQHcqgoulY080-640.jpg","size":640},{"path":"images/amir_houieh-UlQ9CZ1VDQHcqgoulY080-1024.jpg","size":1024}],"caption":null,"alt":"Tomorrow Book-4-image1.jpg","order":4,"r":1.5},{"src":"images/amir_houieh-6CCSZRsudGEAkO8Gqv_cg-320.jpg","srcSet":[{"path":"images/amir_houieh-6CCSZRsudGEAkO8Gqv_cg-320.jpg","size":320},{"path":"images/amir_houieh-6CCSZRsudGEAkO8Gqv_cg-640.jpg","size":640},{"path":"images/amir_houieh-6CCSZRsudGEAkO8Gqv_cg-1024.jpg","size":1024}],"caption":null,"alt":"Tomorrow Book-5-image7.jpg","order":5,"r":1.5}],"stack":["js","html/css"],"dateString":"2015","dataYear":2015,"videos":[]},{"html":"<p>A Data driven spatial installation\nat <a href=\"https://www.kabk.nl\"><code>KABK</code></a>, Den Haag</p>\n<p>Text and Space is a physical translation of the text. I created a system which uses the text as input. It extracts the required data from the text in order to create an space and spacial experiment for the viewers.\nEvery word becomes a dot in a three dimensional space and by connecting the dots a new space is evolved. This project is one of my first attempts working with text and data as material and shaping my visuals through systematic approach.</p>\n<p><a href=\"https://amir.cloud/text_and_space\">See Full Documentation</a></p>\n","title":"Text & Space","description":"<p>Text and Space is a physical translation of the text. I created a system which uses the text as input. It extracts the required data from the text in order to create an space and spacial experiment for the viewers.\nEvery word becomes a dot in a three dimensional space and by connecting the dots a new space is evolved. This project is one of my first attempts working with text and data as material and shaping my visuals through systematic approach.</p>\n","blurb":"Data-driven spatial installation that translates text into 3D space—every word becomes a dot, connecting to form new environments.","tags":["data visualization","installation"],"slug":"text-and-space","thumb":{"src":"images/amir_houieh--PQkG63Zqi3g5brB-y6_P-320.jpg","srcSet":[{"path":"images/amir_houieh--PQkG63Zqi3g5brB-y6_P-320.jpg","size":320},{"path":"images/amir_houieh--PQkG63Zqi3g5brB-y6_P-640.jpg","size":640},{"path":"images/amir_houieh--PQkG63Zqi3g5brB-y6_P-1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-mg_9988-2x.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-mh1IGaHf3pg5Z8Qu2C2c--320.jpg","srcSet":[{"path":"images/amir_houieh-mh1IGaHf3pg5Z8Qu2C2c--320.jpg","size":320},{"path":"images/amir_houieh-mh1IGaHf3pg5Z8Qu2C2c--640.jpg","size":640},{"path":"images/amir_houieh-mh1IGaHf3pg5Z8Qu2C2c--1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-1-00-2x.jpg","order":1,"r":1.5},{"src":"images/amir_houieh-FSGhxPh0qurh0lIc6lQFJ-320.jpg","srcSet":[{"path":"images/amir_houieh-FSGhxPh0qurh0lIc6lQFJ-320.jpg","size":320},{"path":"images/amir_houieh-FSGhxPh0qurh0lIc6lQFJ-640.jpg","size":640},{"path":"images/amir_houieh-FSGhxPh0qurh0lIc6lQFJ-1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-2-01-2x.jpg","order":2,"r":1.5},{"src":"images/amir_houieh-e8n4kF925ngP2OC02tlks-320.jpg","srcSet":[{"path":"images/amir_houieh-e8n4kF925ngP2OC02tlks-320.jpg","size":320},{"path":"images/amir_houieh-e8n4kF925ngP2OC02tlks-640.jpg","size":640},{"path":"images/amir_houieh-e8n4kF925ngP2OC02tlks-1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-3-mg_0047-2x.jpg","order":3,"r":1.5},{"src":"images/amir_houieh-VU_V998FDOjD2MrDQXqHx-320.jpg","srcSet":[{"path":"images/amir_houieh-VU_V998FDOjD2MrDQXqHx-320.jpg","size":320},{"path":"images/amir_houieh-VU_V998FDOjD2MrDQXqHx-640.jpg","size":640},{"path":"images/amir_houieh-VU_V998FDOjD2MrDQXqHx-1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-5-mg_0048-2x.jpg","order":5,"r":1.5},{"src":"images/amir_houieh-K0CbpDwv5-6QSkajFTqKZ-320.jpg","srcSet":[{"path":"images/amir_houieh-K0CbpDwv5-6QSkajFTqKZ-320.jpg","size":320},{"path":"images/amir_houieh-K0CbpDwv5-6QSkajFTqKZ-640.jpg","size":640},{"path":"images/amir_houieh-K0CbpDwv5-6QSkajFTqKZ-1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-6-mg_0061-2x.jpg","order":6,"r":1.5},{"src":"images/amir_houieh-DlNnOSL4yB6ZDfSmf_LCe-320.jpg","srcSet":[{"path":"images/amir_houieh-DlNnOSL4yB6ZDfSmf_LCe-320.jpg","size":320},{"path":"images/amir_houieh-DlNnOSL4yB6ZDfSmf_LCe-640.jpg","size":640},{"path":"images/amir_houieh-DlNnOSL4yB6ZDfSmf_LCe-1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-7-mg_9970-2x.jpg","order":7,"r":1.5},{"src":"images/amir_houieh-ZrO3-newQmUwsG1L3BfPR-320.jpg","srcSet":[{"path":"images/amir_houieh-ZrO3-newQmUwsG1L3BfPR-320.jpg","size":320},{"path":"images/amir_houieh-ZrO3-newQmUwsG1L3BfPR-640.jpg","size":640},{"path":"images/amir_houieh-ZrO3-newQmUwsG1L3BfPR-1024.jpg","size":1024}],"caption":null,"alt":"Text & Space-8-mg_9994-2x.jpg","order":8,"r":1.5}],"stack":null,"dateString":"2014","dataYear":2014,"videos":[]},{"html":"<p><a href=\"https://archive.amir.cloud/1_week_of_my_life_publication\">archive.amir.cloud/1_week_of_my_life_publication</a></p>\n<p>1 week of my life is a series of artworks about one regular week of my life; one work for one day. This series could be consider as a irregular data-visualization representing the data.\nThe dataset was created by formulating my daily schedule into excel tables —categorizing my activities over the time(24hh). In order to make the data more humanized and so more flexible to make narratives, the third parameter was introduced: a parameter which represent my feeling regarding how slow or fast time is passed per activity category.</p>\n<p>This process results into 5 different dataset(s) (Monday to Friday) which are used as content/input for the artworks. You can see the all other related projects at <a href=\"https://archive.amir.cloud\">archive.amir.cloud</a>.\nThis publication is a generated visual book which is a documentation of these series. The book is design and programmed using html, css and javascript in web-browser.</p>\n","title":"1 week of my life / publication","description":"<p>1 week of my life is a series of artworks about one regular week of my life; one work for one day. This series could be consider as a irregular data-visualization representing the data.\nThe dataset was created by formulating my daily schedule into excel tables —categorizing my activities over the time(24hh). In order to make the data more humanized and so more flexible to make narratives, the third parameter was introduced: a parameter which represent my feeling regarding how slow or fast time is passed per activity category.</p>\n","blurb":"Generative visual book documenting one week of life through data visualization—daily schedules visualized with time perception.","tags":["generative book","alternative tool"],"slug":"1-week-of-my-life-publication","thumb":{"src":"images/amir_houieh-sooOhA2VaN4I52qlbWZIp-320.jpg","srcSet":[{"path":"images/amir_houieh-sooOhA2VaN4I52qlbWZIp-320.jpg","size":320},{"path":"images/amir_houieh-sooOhA2VaN4I52qlbWZIp-640.jpg","size":640},{"path":"images/amir_houieh-sooOhA2VaN4I52qlbWZIp-1024.jpg","size":1024}],"caption":null,"alt":"1 week of my life / publication-wednesdays13-2x.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-GXRPhgt0ntitvou2VHRYU-320.jpg","srcSet":[{"path":"images/amir_houieh-GXRPhgt0ntitvou2VHRYU-320.jpg","size":320},{"path":"images/amir_houieh-GXRPhgt0ntitvou2VHRYU-640.jpg","size":640},{"path":"images/amir_houieh-GXRPhgt0ntitvou2VHRYU-1024.jpg","size":1024}],"caption":null,"alt":"1 week of my life / publication-friday13-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-1761477346137gif___.gif","srcSet":[{"path":"images/amir_houieh-1761477346137gif___.gif","size":600}],"caption":"_.gif","alt":"_.gif","order":-1,"r":1.5},{"src":"images/amir_houieh-1761477346140gif____.gif","srcSet":[{"path":"images/amir_houieh-1761477346140gif____.gif","size":600}],"caption":"","alt":"1 week of my life / publication-gif____.gif","order":-1,"r":1.5},{"src":"images/amir_houieh-5fpVE8JtqGXi65rA8pC13-320.jpg","srcSet":[{"path":"images/amir_houieh-5fpVE8JtqGXi65rA8pC13-320.jpg","size":320},{"path":"images/amir_houieh-5fpVE8JtqGXi65rA8pC13-640.jpg","size":640},{"path":"images/amir_houieh-5fpVE8JtqGXi65rA8pC13-1024.jpg","size":1024}],"caption":null,"alt":"1 week of my life / publication-tuesdays13-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-U8ta4K5AaTES3ruE2FX6r-320.jpg","srcSet":[{"path":"images/amir_houieh-U8ta4K5AaTES3ruE2FX6r-320.jpg","size":320},{"path":"images/amir_houieh-U8ta4K5AaTES3ruE2FX6r-640.jpg","size":640},{"path":"images/amir_houieh-U8ta4K5AaTES3ruE2FX6r-1024.jpg","size":1024}],"caption":null,"alt":"1 week of my life / publication-wednesdays13-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-dZNNjqJCyEhQQ01Oo0GXn-320.jpg","srcSet":[{"path":"images/amir_houieh-dZNNjqJCyEhQQ01Oo0GXn-320.jpg","size":320},{"path":"images/amir_houieh-dZNNjqJCyEhQQ01Oo0GXn-640.jpg","size":640},{"path":"images/amir_houieh-dZNNjqJCyEhQQ01Oo0GXn-1024.jpg","size":1024}],"caption":null,"alt":"1 week of my life / publication-wednesdays2-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-ei0Q-EN3-7QJFD1R4X5Ma-320.jpg","srcSet":[{"path":"images/amir_houieh-ei0Q-EN3-7QJFD1R4X5Ma-320.jpg","size":320},{"path":"images/amir_houieh-ei0Q-EN3-7QJFD1R4X5Ma-640.jpg","size":640},{"path":"images/amir_houieh-ei0Q-EN3-7QJFD1R4X5Ma-1024.jpg","size":1024}],"caption":null,"alt":"1 week of my life / publication-wednesdays8-2x.jpg","order":-1,"r":1.5}],"stack":["js","html/css"],"dateString":"2013","dataYear":2013,"videos":[]},{"html":"<p><a href=\"https://archive.amir.cloud/2001_a_space_odyssey_round_1\">2001_a_space_odyssey_round_1</a>\n<a href=\"https://archive.amir.cloud/2001_a_space_odyssey_round_2\">2001_a_space_odyssey_round_2</a>\n<a href=\"https://archive.amir.cloud/2001_a_space_odyssey_round_3\">2001_a_space_odyssey_round_3</a>\n<a href=\"https://archive.amir.cloud/2001_a_space_odyssey_round_4\">2001_a_space_odyssey_round_4</a></p>\n<p>2001 a space odyssey is a poster assignment — by <a href=\"http://carvalho-bernau.com\">Susana Carvalho</a> — consist of 4 phases/iteration. While the research phase the concepts such as construction and deconstruction and recursive loops came to my focus point. Furthermore I discovered about the technique used by film in order to create a sound in one of the famous sequences. They used a audio feedback loop where speaker outputs the sound which it creates.\nHence this concept lead me a to design a system in which I could create my poster with: a visual feedback loop. The setup consist of a typography layer of title of the movie (as the initial material/input) and a tv screen (to output the visuals) and a camera (to catch the visual input).</p>\n","title":"2001 a space odyssey / poster series","description":"<p>2001 a space odyssey is a poster assignment — by <a href=\"http://carvalho-bernau.com\">Susana Carvalho</a> — consist of 4 phases/iteration. While the research phase the concepts such as construction and deconstruction and recursive loops came to my focus point. Furthermore I discovered about the technique used by film in order to create a sound in one of the famous sequences. They used a audio feedback loop where speaker outputs the sound which it creates.\nHence this concept lead me a to design a system in which I could create my poster with: a visual feedback loop. The setup consist of a typography layer of title of the movie (as the initial material/input) and a tv screen (to output the visuals) and a camera (to catch the visual input).</p>\n","blurb":"Generative poster series using visual feedback loops—camera captures screen output to create recursive, evolving visuals.","tags":["generative poster","alternative tool"],"slug":"2001-a-space-odyssey-poster-series","thumb":{"src":"images/amir_houieh-_GmO9UvbJXfJyiVssuewN-320.jpg","srcSet":[{"path":"images/amir_houieh-_GmO9UvbJXfJyiVssuewN-320.jpg","size":320},{"path":"images/amir_houieh-_GmO9UvbJXfJyiVssuewN-640.jpg","size":640},{"path":"images/amir_houieh-_GmO9UvbJXfJyiVssuewN-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_6796_copy-2x.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-wf0xgw6Lrh6Phx2BobTXZ-320.jpg","srcSet":[{"path":"images/amir_houieh-wf0xgw6Lrh6Phx2BobTXZ-320.jpg","size":320},{"path":"images/amir_houieh-wf0xgw6Lrh6Phx2BobTXZ-640.jpg","size":640},{"path":"images/amir_houieh-wf0xgw6Lrh6Phx2BobTXZ-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_6752_copy-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-MsoAdBx1BEkNKmriVd7fc-320.jpg","srcSet":[{"path":"images/amir_houieh-MsoAdBx1BEkNKmriVd7fc-320.jpg","size":320},{"path":"images/amir_houieh-MsoAdBx1BEkNKmriVd7fc-640.jpg","size":640},{"path":"images/amir_houieh-MsoAdBx1BEkNKmriVd7fc-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_6796_copy-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-UJvrVYwJQNb4aMJ4AkKUn-320.jpg","srcSet":[{"path":"images/amir_houieh-UJvrVYwJQNb4aMJ4AkKUn-320.jpg","size":320},{"path":"images/amir_houieh-UJvrVYwJQNb4aMJ4AkKUn-640.jpg","size":640},{"path":"images/amir_houieh-UJvrVYwJQNb4aMJ4AkKUn-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_6896_copy-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-KJywq8Ba9XsI-jgki_ikB-320.jpg","srcSet":[{"path":"images/amir_houieh-KJywq8Ba9XsI-jgki_ikB-320.jpg","size":320},{"path":"images/amir_houieh-KJywq8Ba9XsI-jgki_ikB-640.jpg","size":640},{"path":"images/amir_houieh-KJywq8Ba9XsI-jgki_ikB-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_6903_copy-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-iwR7MRpeGDH1rfKoI9hjx-320.jpg","srcSet":[{"path":"images/amir_houieh-iwR7MRpeGDH1rfKoI9hjx-320.jpg","size":320},{"path":"images/amir_houieh-iwR7MRpeGDH1rfKoI9hjx-640.jpg","size":640},{"path":"images/amir_houieh-iwR7MRpeGDH1rfKoI9hjx-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_6940_copy-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-vqGOSq1u3ez8HxGsuGo__-320.jpg","srcSet":[{"path":"images/amir_houieh-vqGOSq1u3ez8HxGsuGo__-320.jpg","size":320},{"path":"images/amir_houieh-vqGOSq1u3ez8HxGsuGo__-640.jpg","size":640},{"path":"images/amir_houieh-vqGOSq1u3ez8HxGsuGo__-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_7168_copy-2x.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-zGUKtrsvI2kOD8KHNZUuP-320.jpg","srcSet":[{"path":"images/amir_houieh-zGUKtrsvI2kOD8KHNZUuP-320.jpg","size":320},{"path":"images/amir_houieh-zGUKtrsvI2kOD8KHNZUuP-640.jpg","size":640},{"path":"images/amir_houieh-zGUKtrsvI2kOD8KHNZUuP-1024.jpg","size":1024}],"caption":null,"alt":"2001 a space odyssey / poster series-_mg_7206-2x.jpg","order":-1,"r":1.5}],"stack":null,"dateString":"2013","dataYear":2013,"videos":[]},{"html":"<h1>Noui project</h1>\n<p>Noui project is a series of experiments towards my vision and definition of the future of UI/UX design in software development; First, adaptability and organic personalization of interface design of software and second Automation and the design process. </p>\n<p><strong>Experiment #1</strong></p>\n<p>The first experiment was a StyleGAN2 generated web grid, an attempt to see how we can teach a GAN to draw a random grid(layout) of a website. For this experiment, I used a dataset including the top 1000 websites with the most daily views. I am not quite happy with the results but now that the model is trained, it can be used for further (continue) training and hopefully get more coherent results. </p>\n<p>In order to create this dataset, I had to develop the software ⇢ <a href=\"http://gridr.amir.cloud/\">http://gridr.amir.cloud/</a>. The source code is open-source on Github ⇢ <a href=\"https://github.com/amirhouieh/grid-reveal\">https://github.com/amirhouieh/grid-reveal</a>. </p>\n<p><strong>Experiment #2</strong></p>\n<p>The second experiment was a WIREFRAME to UI generator using Pix2Pix GAN. In this step, I used a dataset including 1000 random data points (webpages). The result did not look good, but promising 🤞 The next step would be to create a more cohesive and clean dataset (probably limited to one particular domain), and train the model again. </p>\n<p>Link to Colab project ⇢ </p>\n<p><a href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb\">Google Colaboratory</a></p>\n","title":"noui","description":"<p>The first experiment was a StyleGAN2 generated web grid, an attempt to see how we can teach a GAN to draw a random grid(layout) of a website. For this experiment, I used a dataset including the top 1000 websites with the most daily views. I am not quite happy with the results but now that the model is trained, it can be used for further (continue) training and hopefully get more coherent results. </p>\n","blurb":"Machine learning experiments exploring the future of UI/UX design using StyleGAN2 and Pix2Pix for automated interface generation.","tags":["Experiment","Machine Learning"],"slug":"noui","thumb":{"src":"images/amir_houieh-1761477346085ezgif-2-d790860933.gif","srcSet":[{"path":"images/amir_houieh-1761477346085ezgif-2-d790860933.gif","size":600}],"caption":null,"alt":"noui-ezgif-2-d790860933.gif","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-1761477346127ezgif-2-a57c9da08d.gif","srcSet":[{"path":"images/amir_houieh-1761477346127ezgif-2-a57c9da08d.gif","size":600}],"caption":null,"alt":"noui-ezgif-2-a57c9da08d.gif","order":-1,"r":1.5}],"stack":["StyleGan2","Pix2Pix","Python"],"dateString":"2020","dataYear":2020,"videos":[],"order":1322},{"html":"<p><a href=\"https://projectindefensible.org\">projectindefensible.org</a></p>\n<p><code>with</code> <a href=\"https://lust.nl\"><code>LUST/LUSTLAB</code></a>, <a href=\"https://eric.young.li\"><code>Eric Li</code></a>, <a href=\"https://linkedin.com/in/lenarobin\"><code>Léna Robin</code></a></p>\n<p>Project Indefensible emerged out of a series of research and discussion workshops on the global arms business. The result of this collaboration is the educational website and text of Indefensible. In order to stimulate this conversation, <a href=\"https://lust.nl\">LUST</a>, where I worked as a designer and developer, approached the design and implementation of the website as a critical tool for collaborative reading and writing. LUST offered speculations on scalable reading experiences between all target audiences, situations, and locations centered on the thematics of the Indefensible text.\nMy contribution to this project was mainly implementation and software development including both back-end and front-end. As a result, the website for Indefensible offers the reader a set of tools which you can find at the online website.</p>\n","title":"Project Indefensible","description":"<p>Project Indefensible emerged out of a series of research and discussion workshops on the global arms business. The result of this collaboration is the educational website and text of Indefensible. In order to stimulate this conversation, <a href=\"https://lust.nl\">LUST</a>, where I worked as a designer and developer, approached the design and implementation of the website as a critical tool for collaborative reading and writing. LUST offered speculations on scalable reading experiences between all target audiences, situations, and locations centered on the thematics of the Indefensible text.\nMy contribution to this project was mainly implementation and software development including both back-end and front-end. As a result, the website for Indefensible offers the reader a set of tools which you can find at the online website.</p>\n","blurb":"Educational website on the global arms business with collaborative reading and writing tools—critical publishing platform.","tags":["website","publishing platform"],"slug":"project-indefensible","thumb":{"src":"images/amir_houieh-Ppo4WM23QdayAh_n_uI1A-320.jpg","srcSet":[{"path":"images/amir_houieh-Ppo4WM23QdayAh_n_uI1A-320.jpg","size":320},{"path":"images/amir_houieh-Ppo4WM23QdayAh_n_uI1A-640.jpg","size":640},{"path":"images/amir_houieh-Ppo4WM23QdayAh_n_uI1A-1024.jpg","size":1024}],"caption":null,"alt":"Project Indefensible-indefensible.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-ZHGZUnUD1ITAlhQDovNrD-320.png","srcSet":[{"path":"images/amir_houieh-ZHGZUnUD1ITAlhQDovNrD-320.png","size":320},{"path":"images/amir_houieh-ZHGZUnUD1ITAlhQDovNrD-640.png","size":640},{"path":"images/amir_houieh-ZHGZUnUD1ITAlhQDovNrD-1024.png","size":1024}],"caption":"highlighting and note taking","alt":"highlighting and note taking","order":-1,"r":1.5},{"src":"images/amir_houieh-vSO4tx-MniatqvvKWr8Xo-320.png","srcSet":[{"path":"images/amir_houieh-vSO4tx-MniatqvvKWr8Xo-320.png","size":320},{"path":"images/amir_houieh-vSO4tx-MniatqvvKWr8Xo-640.png","size":640},{"path":"images/amir_houieh-vSO4tx-MniatqvvKWr8Xo-1024.png","size":1024}],"caption":"search","alt":"search","order":-1,"r":1.5}],"stack":["js","html/css"],"dateString":"2016","dataYear":2016,"videos":[],"order":999},{"html":"<p><a href=\"https://isiaurbino.net\">isiaurbino.net</a></p>\n<p><code>with</code> <a href=\"http://zerodotzero.net\"><code>Zero Dot Zero</code></a></p>\n<p>The project is a result of about 6 months of dedicated collaboration with Zero Dot Zero as a continuation to  our previous project in the field of #experimentalpublishing and #digitalpublishing.\nThis platform uses Google Drive and Google Calendar as CMS; users produce, edit and upload their content using GDoc, GSpreadsheets, GCal and/or any other file formats in Gdrive, whereupon the software processes and exposes the content publicly via two APIs #Restful &amp; #GraphQl).</p>\n<p>Accreditation:</p>\n<p>ISIA Urbino Identity:\nZeroDotZero</p>\n<p>Software Architecture &amp; Development:\nAmir Houieh</p>\n<p>Concept &amp; Design:\nMaaike Besseling, Thomas Castro, Michiel Terpelle</p>\n<p>Identity Implementation:\nMaloe Brinkman, Thomas Castro, Jurrit van der Ploeg</p>\n<p>Coordination:\nWietske Flederus, Floor Weijs</p>\n","title":"ISIA Urbino Digital Platform","description":"<p>The project is a result of about 6 months of dedicated collaboration with Zero Dot Zero as a continuation to  our previous project in the field of #experimentalpublishing and #digitalpublishing.\nThis platform uses Google Drive and Google Calendar as CMS; users produce, edit and upload their content using GDoc, GSpreadsheets, GCal and/or any other file formats in Gdrive, whereupon the software processes and exposes the content publicly via two APIs #Restful &amp; #GraphQl).</p>\n","blurb":"Publishing platform using Google Drive as CMS—content managed via GDocs/Sheets, exposed via REST & GraphQL APIs.","tags":["website","publishing platform"],"slug":"isia-urbino-digital-platform","thumb":{"src":"images/amir_houieh-2sVXPRxnQceDvrLYFzeMJ-320.jpg","srcSet":[{"path":"images/amir_houieh-2sVXPRxnQceDvrLYFzeMJ-320.jpg","size":320},{"path":"images/amir_houieh-2sVXPRxnQceDvrLYFzeMJ-640.jpg","size":640},{"path":"images/amir_houieh-2sVXPRxnQceDvrLYFzeMJ-1024.jpg","size":1024}],"caption":null,"alt":"ISIA Urbino Digital Platform-render-2.jpg","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-x0seExhKMAT2ewmfQIa1a-320.jpg","srcSet":[{"path":"images/amir_houieh-x0seExhKMAT2ewmfQIa1a-320.jpg","size":320},{"path":"images/amir_houieh-x0seExhKMAT2ewmfQIa1a-640.jpg","size":640},{"path":"images/amir_houieh-x0seExhKMAT2ewmfQIa1a-1024.jpg","size":1024}],"caption":null,"alt":"ISIA Urbino Digital Platform-render-3.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-N6XzngPtpJxYt-_yBFq_Z-320.jpg","srcSet":[{"path":"images/amir_houieh-N6XzngPtpJxYt-_yBFq_Z-320.jpg","size":320},{"path":"images/amir_houieh-N6XzngPtpJxYt-_yBFq_Z-640.jpg","size":640},{"path":"images/amir_houieh-N6XzngPtpJxYt-_yBFq_Z-1024.jpg","size":1024}],"caption":null,"alt":"ISIA Urbino Digital Platform-render-4.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-kUh4sHnY9cJjzxdiRYzUG-320.jpg","srcSet":[{"path":"images/amir_houieh-kUh4sHnY9cJjzxdiRYzUG-320.jpg","size":320},{"path":"images/amir_houieh-kUh4sHnY9cJjzxdiRYzUG-640.jpg","size":640},{"path":"images/amir_houieh-kUh4sHnY9cJjzxdiRYzUG-1024.jpg","size":1024}],"caption":null,"alt":"ISIA Urbino Digital Platform-render-5.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-BSqml9FtfKNsD677guDmH-320.jpg","srcSet":[{"path":"images/amir_houieh-BSqml9FtfKNsD677guDmH-320.jpg","size":320},{"path":"images/amir_houieh-BSqml9FtfKNsD677guDmH-640.jpg","size":640},{"path":"images/amir_houieh-BSqml9FtfKNsD677guDmH-1024.jpg","size":1024}],"caption":null,"alt":"ISIA Urbino Digital Platform-render-6.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-SBUbjFGwy3il3AB1pucNZ-320.jpg","srcSet":[{"path":"images/amir_houieh-SBUbjFGwy3il3AB1pucNZ-320.jpg","size":320},{"path":"images/amir_houieh-SBUbjFGwy3il3AB1pucNZ-640.jpg","size":640},{"path":"images/amir_houieh-SBUbjFGwy3il3AB1pucNZ-1024.jpg","size":1024}],"caption":null,"alt":"ISIA Urbino Digital Platform-render-7.jpg","order":-1,"r":1.5},{"src":"images/amir_houieh-YTpHOdY8_PAYzReSk8wQi-320.jpg","srcSet":[{"path":"images/amir_houieh-YTpHOdY8_PAYzReSk8wQi-320.jpg","size":320},{"path":"images/amir_houieh-YTpHOdY8_PAYzReSk8wQi-640.jpg","size":640},{"path":"images/amir_houieh-YTpHOdY8_PAYzReSk8wQi-1024.jpg","size":1024}],"caption":null,"alt":"ISIA Urbino Digital Platform-render-8.jpg","order":-1,"r":1.5}],"stack":["React","html/css","Typescript","Nodejs","GraphQl","Google API"],"dateString":"2020","dataYear":2020,"videos":[],"order":888},{"html":"<p><a href=\"https://25thhour.rndr.studio\">25thhour</a></p>\n<p>This is one of the projects which I worked on at RNDR.</p>\n<p>25th Hour: Flow is a project by Audi in partnership with the Karlsruhe Institute for Technology (KIT), MobilityPartners and <a href=\"https://rndr.studio\">RNDR</a>. Based on data obtained from a simulation conducted by KIT and MobilityPartners, 25th Hour visualizes the potential impact of autonomous cars, ride sharing and smart traffic management on the traffic of Ingolstadt, Germany.</p>\n","title":"The 25th Hour","description":"<p>25th Hour: Flow is a project by Audi in partnership with the Karlsruhe Institute for Technology (KIT), MobilityPartners and <a href=\"https://rndr.studio\">RNDR</a>. Based on data obtained from a simulation conducted by KIT and MobilityPartners, 25th Hour visualizes the potential impact of autonomous cars, ride sharing and smart traffic management on the traffic of Ingolstadt, Germany.</p>\n","blurb":"WebGL visualization of autonomous car impact on traffic flow in Ingolstadt—project by Audi in partnership with KIT.","tags":["data visualization","webGl","map visualization"],"slug":"the-25th-hour","thumb":{"src":"images/amir_houieh-13lALK65RQO_Y_p4F22id-320.png","srcSet":[{"path":"images/amir_houieh-13lALK65RQO_Y_p4F22id-320.png","size":320},{"path":"images/amir_houieh-13lALK65RQO_Y_p4F22id-640.png","size":640},{"path":"images/amir_houieh-13lALK65RQO_Y_p4F22id-1024.png","size":1024}],"caption":null,"alt":"The 25th Hour-Screen Shot 2019-03-12 at 16.05.08.png","order":-1,"r":1.5},"images":[],"stack":["js","html/css","TypeScript","React","Mapbox","Kotlin","OPENRNDR"],"dateString":"2018","dataYear":2018,"videos":[{"src":"https://vimeo.com/311687534","source":"vimeo","caption":null,"order":2}],"order":777},{"html":"<p><em>A modular content-driven web browser</em>\n<a href=\"https://github.com/amirhouieh/re-\">source code on Github</a></p>\n<p>Re is my graduation project, which already contains some of the key ideas of the Repub project. Re is a new web browser, altering the way we surf and consume on the web. Re empowers the user to embrace the content of the web pages as well as the design, empowering them to reflect on the way content is shown. Re is a modular platform within which users can decide what type of content they want to see on a webpage\nThis is enabled by creating and adding content modules to the browser. Therefore, Re becomes a super minimal browser which is faster, more reliable, and more efficient in terms of data-consumption by cutting off all the noises from a webpage’s content.</p>\n","title":"Re-","description":"<p>Re is my graduation project, which already contains some of the key ideas of the Repub project. Re is a new web browser, altering the way we surf and consume on the web. Re empowers the user to embrace the content of the web pages as well as the design, empowering them to reflect on the way content is shown. Re is a modular platform within which users can decide what type of content they want to see on a webpage\nThis is enabled by creating and adding content modules to the browser. Therefore, Re becomes a super minimal browser which is faster, more reliable, and more efficient in terms of data-consumption by cutting off all the noises from a webpage’s content.</p>\n","blurb":"Modular web browser that empowers users to customize content consumption and cut through webpage noise with content modules.","tags":["experimental publishing","software","DIY tool"],"slug":"re","thumb":{"src":"images/amir_houieh-syvKZqPuJawu2zZuLh1TJ-320.png","srcSet":[{"path":"images/amir_houieh-syvKZqPuJawu2zZuLh1TJ-320.png","size":320},{"path":"images/amir_houieh-syvKZqPuJawu2zZuLh1TJ-640.png","size":640},{"path":"images/amir_houieh-syvKZqPuJawu2zZuLh1TJ-1024.png","size":1024}],"caption":null,"alt":"Re--re-browser-homepage.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-W2k3Bl-TV6ZD_XNT0SYvO-320.png","srcSet":[{"path":"images/amir_houieh-W2k3Bl-TV6ZD_XNT0SYvO-320.png","size":320},{"path":"images/amir_houieh-W2k3Bl-TV6ZD_XNT0SYvO-640.png","size":640},{"path":"images/amir_houieh-W2k3Bl-TV6ZD_XNT0SYvO-1024.png","size":1024}],"caption":"How a Wikipedia page could look like in Re","alt":"How a Wikipedia page could look like in Re","order":1,"r":1.5},{"src":"images/amir_houieh-UEgHJc1pJ-WrlFGQD6GKk-320.png","srcSet":[{"path":"images/amir_houieh-UEgHJc1pJ-WrlFGQD6GKk-320.png","size":320},{"path":"images/amir_houieh-UEgHJc1pJ-WrlFGQD6GKk-640.png","size":640},{"path":"images/amir_houieh-UEgHJc1pJ-WrlFGQD6GKk-1024.png","size":1024}],"caption":"How Google search could look like in Re","alt":"How Google search could look like in Re","order":2,"r":1.5},{"src":"images/amir_houieh-JBPLI9JSy2HjkPhTW2jBw-320.png","srcSet":[{"path":"images/amir_houieh-JBPLI9JSy2HjkPhTW2jBw-320.png","size":320},{"path":"images/amir_houieh-JBPLI9JSy2HjkPhTW2jBw-640.png","size":640},{"path":"images/amir_houieh-JBPLI9JSy2HjkPhTW2jBw-1024.png","size":1024}],"caption":"How KABK.nl could look like in Re","alt":"How KABK.nl could look like in Re","order":3,"r":1.5},{"src":"images/amir_houieh-tutmYGaSVgJ02Xk_DZv6h-320.png","srcSet":[{"path":"images/amir_houieh-tutmYGaSVgJ02Xk_DZv6h-320.png","size":320},{"path":"images/amir_houieh-tutmYGaSVgJ02Xk_DZv6h-640.png","size":640},{"path":"images/amir_houieh-tutmYGaSVgJ02Xk_DZv6h-1024.png","size":1024}],"caption":"General UI of Re - menu","alt":"General UI of Re - menu","order":4,"r":1.5},{"src":"images/amir_houieh-4D9va0ZnijrE_xNN8rzS6-320.png","srcSet":[{"path":"images/amir_houieh-4D9va0ZnijrE_xNN8rzS6-320.png","size":320},{"path":"images/amir_houieh-4D9va0ZnijrE_xNN8rzS6-640.png","size":640},{"path":"images/amir_houieh-4D9va0ZnijrE_xNN8rzS6-1024.png","size":1024}],"caption":"General UI of Re - browser history","alt":"General UI of Re - browser history","order":5,"r":1.5},{"src":"images/amir_houieh-BC8d2MO4eUB-vW564zPDs-320.png","srcSet":[{"path":"images/amir_houieh-BC8d2MO4eUB-vW564zPDs-320.png","size":320},{"path":"images/amir_houieh-BC8d2MO4eUB-vW564zPDs-640.png","size":640},{"path":"images/amir_houieh-BC8d2MO4eUB-vW564zPDs-1024.png","size":1024}],"caption":"General UI of Re - shortcuts","alt":"General UI of Re - shortcuts","order":6,"r":1.5}],"stack":["js","html/css","electron-js"],"dateString":"2016","dataYear":2016,"videos":[],"order":669},{"html":"<p><a href=\"https://pagesmagazine.net\">pagesmagazine.net</a></p>\n<p><code>with</code> <a href=\"https://lust.nl\"><code>LUST/LUSTLAB</code></a>, <a href=\"https://linkedin.com/in/lenarobin\"><code>Léna Robin</code></a>, <a href=\"https://krks.info/\"><code>Gabor Kerekes</code></a></p>\n<p>This project is the online expansion of the publishing work of Pages, the bilingual, Farsi and English, artist magazine. This platform operates primarily as a working space, concerned with rethinking the politics and practice of archiving and publishing. It approaches publishing as a collective practice of generating an open, transparent archive. With minimum editorial mediation, the platform offers direct access to invited authors to publish their contributions, while making them instantly accessible to the reader. The central tool facilitating this is the automated mailing system which authors use to upload, edit, and expand on their contributions.\nI was proudly involved in the research, design, and implementation phase of this project during my time at LUST/LustLab. I can honestly say that the Pages project was my best working experience during this time for two reasons: Firstly, because the project was about online publishing, a topic of special interest to me and secondly, because of my major position within the project which brought me a lot of responsibilities as well as challenges.</p>\n","title":"Pages","description":"<p>This project is the online expansion of the publishing work of Pages, the bilingual, Farsi and English, artist magazine. This platform operates primarily as a working space, concerned with rethinking the politics and practice of archiving and publishing. It approaches publishing as a collective practice of generating an open, transparent archive. With minimum editorial mediation, the platform offers direct access to invited authors to publish their contributions, while making them instantly accessible to the reader. The central tool facilitating this is the automated mailing system which authors use to upload, edit, and expand on their contributions.\nI was proudly involved in the research, design, and implementation phase of this project during my time at LUST/LustLab. I can honestly say that the Pages project was my best working experience during this time for two reasons: Firstly, because the project was about online publishing, a topic of special interest to me and secondly, because of my major position within the project which brought me a lot of responsibilities as well as challenges.</p>\n","blurb":"Bilingual publishing platform for collaborative archiving with automated mailing system and direct author access.","tags":["website","publishing platform"],"slug":"pages","thumb":{"src":"images/amir_houieh-yiMLZKWbGt15xttqN9dC9-320.png","srcSet":[{"path":"images/amir_houieh-yiMLZKWbGt15xttqN9dC9-320.png","size":320},{"path":"images/amir_houieh-yiMLZKWbGt15xttqN9dC9-640.png","size":640},{"path":"images/amir_houieh-yiMLZKWbGt15xttqN9dC9-1024.png","size":1024}],"caption":null,"alt":"Pages-pagesmagazine.net.png","order":-1,"r":1.5},"images":[{"src":"images/amir_houieh-ypjUvnkH6UOMgPDuBECjZ-320.png","srcSet":[{"path":"images/amir_houieh-ypjUvnkH6UOMgPDuBECjZ-320.png","size":320},{"path":"images/amir_houieh-ypjUvnkH6UOMgPDuBECjZ-640.png","size":640},{"path":"images/amir_houieh-ypjUvnkH6UOMgPDuBECjZ-1024.png","size":1024}],"caption":"Pages platform has it’s own search engine, which enables the users to search among different type of content within its database","alt":"Pages platform has it’s own search engine, which enables the users to search among different type of content within its database","order":1,"r":1.5},{"src":"images/amir_houieh-BiPbk8xZjbOkq4pqplLCh-320.png","srcSet":[{"path":"images/amir_houieh-BiPbk8xZjbOkq4pqplLCh-320.png","size":320},{"path":"images/amir_houieh-BiPbk8xZjbOkq4pqplLCh-640.png","size":640},{"path":"images/amir_houieh-BiPbk8xZjbOkq4pqplLCh-1024.png","size":1024}],"caption":"Versioning for articles and other types of content","alt":"Versioning for articles and other types of content","order":2,"r":1.5},{"src":"images/amir_houieh-iAkmBA267Pne7c72s1emi-320.png","srcSet":[{"path":"images/amir_houieh-iAkmBA267Pne7c72s1emi-320.png","size":320},{"path":"images/amir_houieh-iAkmBA267Pne7c72s1emi-640.png","size":640},{"path":"images/amir_houieh-iAkmBA267Pne7c72s1emi-1024.png","size":1024}],"caption":"Users are able to create their own magazines  by adding:removing articles to their collection. Further the platform enables them to publish their collection with other users and:or generate a designed PDF version","alt":"Users are able to create their own magazines  by adding:removing articles to their collection. Further the platform enables them to publish their collection with other users and:or generate a designed PDF version","order":3,"r":1.5},{"src":"images/amir_houieh-2rBgKB4OZ9KNGV1Zv82fn-320.png","srcSet":[{"path":"images/amir_houieh-2rBgKB4OZ9KNGV1Zv82fn-320.png","size":320},{"path":"images/amir_houieh-2rBgKB4OZ9KNGV1Zv82fn-640.png","size":640},{"path":"images/amir_houieh-2rBgKB4OZ9KNGV1Zv82fn-1024.png","size":1024}],"caption":null,"alt":"Pages-4-Screen Shot 2018-09-30 at 22.22.51.png","order":4,"r":1.5},{"src":"images/amir_houieh-dWHBicnq9bs2Gd-Z0V8Yi-320.png","srcSet":[{"path":"images/amir_houieh-dWHBicnq9bs2Gd-Z0V8Yi-320.png","size":320},{"path":"images/amir_houieh-dWHBicnq9bs2Gd-Z0V8Yi-640.png","size":640},{"path":"images/amir_houieh-dWHBicnq9bs2Gd-Z0V8Yi-1024.png","size":1024}],"caption":null,"alt":"Pages-5-Screen Shot 2018-09-30 at 22.23.16.png","order":5,"r":1.5},{"src":"images/amir_houieh-0N3dpl_89Nrdu-BQtFD62-320.png","srcSet":[{"path":"images/amir_houieh-0N3dpl_89Nrdu-BQtFD62-320.png","size":320},{"path":"images/amir_houieh-0N3dpl_89Nrdu-BQtFD62-640.png","size":640},{"path":"images/amir_houieh-0N3dpl_89Nrdu-BQtFD62-1024.png","size":1024}],"caption":null,"alt":"Pages-6-Screen Shot 2018-09-30 at 22.23.24.png","order":6,"r":1.5}],"stack":["es6","html/css","React","Nodejs","docker","monogo-DB","express-js","smtp-server"],"dateString":"2016-2017","dataYear":2017,"videos":[],"order":666}]}